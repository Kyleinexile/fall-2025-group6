{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bef78b",
   "metadata": {},
   "source": [
    "# LAiSER K/S/A Extraction Pipeline for Military AFSCs\n",
    "\n",
    "## Overview\n",
    "This pipeline extracts Knowledge, Skills, and Abilities (K/S/A) from military Air Force Specialty Code (AFSC) descriptions using the LAiSER (Leveraging AI for Skill Extraction & Research) framework.\n",
    "\n",
    "### Key Features:\n",
    "- Processes 12 AFSC descriptions from Operations, Intelligence, and Maintenance categories\n",
    "- Extracts skills using ESCO taxonomy via LAiSER\n",
    "- Classifies extractions into Knowledge, Skills, or Abilities\n",
    "- Generates evidence snippets for traceability\n",
    "- Applies quality filters (confidence threshold, stoplist)\n",
    "- Produces QC samples for validation\n",
    "- Exports graph-ready structure for database import\n",
    "\n",
    "### Pipeline Outputs:\n",
    "1. `ksa_extractions.csv` - Filtered K/S/A items with metadata\n",
    "2. `qc_sample.csv` - Stratified sample for quality control\n",
    "3. `extraction_stats.json` - Summary statistics\n",
    "4. `graph_export.json` - Graph database structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0d0f9",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup and Configuration\n",
    "\n",
    "This section imports required libraries and sets up the pipeline configuration.\n",
    "\n",
    "### Key Components:\n",
    "- **Path Configuration**: Sets input/output directories for Windows environment\n",
    "- **Quality Thresholds**: MIN_CONFIDENCE (0.55) filters low-quality extractions\n",
    "- **Generic Stoplist**: Removes irrelevant terms (navy/maritime references)\n",
    "- **KSA Patterns**: Keywords for classifying extracted items into K/S/A categories\n",
    "\n",
    "The configuration is deliberately minimal to maintain simplicity and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0638acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized at 2025-09-30 09:38:03\n",
      "Output directory: C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_output_simple\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from laiser.skill_extractor import Skill_Extractor\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\")\n",
    "INPUT_FILE = DATA_DIR / \"corpus_manual_dataset.jsonl\"\n",
    "OUTPUT_DIR = DATA_DIR / \"ksa_output_simple\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Quality thresholds\n",
    "MIN_CONFIDENCE = 0.55\n",
    "GENERIC_STOPLIST = [\n",
    "    \"navy operations\", \"naval operations\", \"small vessel\",\n",
    "    \"maritime\", \"marine operations\", \"ship operations\"\n",
    "]\n",
    "\n",
    "# K/S/A classification patterns\n",
    "KSA_PATTERNS = {\n",
    "    \"knowledge\": [\"knowledge\", \"understand\", \"theory\", \"principles\", \"concepts\", \n",
    "                  \"awareness\", \"familiarity\"],\n",
    "    \"skill\": [\"operate\", \"perform\", \"execute\", \"conduct\", \"implement\", \n",
    "              \"maintain\", \"repair\", \"troubleshoot\", \"analyze\"],\n",
    "    \"ability\": [\"able to\", \"capability\", \"capacity\", \"aptitude\", \"competence\", \n",
    "                \"proficiency\", \"adapt\", \"lead\", \"manage\", \"coordinate\"]\n",
    "}\n",
    "\n",
    "print(f\"Pipeline initialized at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e93b2",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Preparation\n",
    "\n",
    "### Functions:\n",
    "- `load_jsonl()`: Loads the JSONL dataset containing AFSC descriptions\n",
    "- Data preparation: Renames columns to match LAiSER's expected format\n",
    "  - `doc_id` â†’ `job_id` (unique identifier)\n",
    "  - `text` â†’ `description` (main content for extraction)\n",
    "\n",
    "### Preprocessing:\n",
    "- Minimal text cleaning\n",
    "- No complex transformations to preserve original military terminology\n",
    "- Validates required columns are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a822e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 12 AFSC records from corpus_manual_dataset.jsonl\n",
      "âœ“ Data prepared for LAiSER extraction\n",
      "  Categories: ['Operations' 'Intelligence' 'Maintenance']\n",
      "  Sample AFSC: 11F3 - FIGHTER PILOT\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(fp: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load JSONL file into DataFrame\"\"\"\n",
    "    rows = []\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Load dataset\n",
    "df = load_jsonl(INPUT_FILE)\n",
    "print(f\"âœ“ Loaded {len(df)} AFSC records from {INPUT_FILE.name}\")\n",
    "\n",
    "# Validate columns\n",
    "required_cols = {\"doc_id\", \"text\", \"title\", \"afsc\", \"category\"}\n",
    "assert required_cols.issubset(df.columns), f\"Missing columns: {required_cols - set(df.columns)}\"\n",
    "\n",
    "# Prepare for LAiSER\n",
    "laiser_df = df[[\"doc_id\", \"text\", \"title\", \"afsc\", \"category\"]].rename(\n",
    "    columns={\"doc_id\": \"job_id\", \"text\": \"description\"}\n",
    ")\n",
    "\n",
    "# Minimal preprocessing\n",
    "laiser_df[\"description\"] = laiser_df[\"description\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "print(f\"âœ“ Data prepared for LAiSER extraction\")\n",
    "print(f\"  Categories: {laiser_df['category'].unique()}\")\n",
    "print(f\"  Sample AFSC: {laiser_df.iloc[0]['afsc']} - {laiser_df.iloc[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b2f3e",
   "metadata": {},
   "source": [
    "## Section 3: LAiSER Initialization\n",
    "\n",
    "### Model Configuration:\n",
    "- **Model**: Microsoft Phi-2 (2.7B parameters, CPU-optimized)\n",
    "- **Mode**: CPU-only (avoids vLLM/GPU dependencies on Windows)\n",
    "- **Authentication**: Requires Hugging Face token for model access\n",
    "\n",
    "### Phi-2\n",
    "- Efficient for CPU processing\n",
    "- Good balance of speed and accuracy\n",
    "- Successfully processes 12 AFSCs in ~0.21 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf432425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing LAiSER with microsoft/phi-2...\n",
      "Loading ESCO skill taxonomy data...\n",
      "Loading FAISS index for ESCO skills...\n",
      "FAISS index for ESCO skills loaded successfully.\n",
      "Found 'en_core_web_lg' model. Loading...\n",
      "GPU is not available. Using CPU for SkillNer model initialization...\n",
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n",
      "âœ“ LAiSER initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Get credentials\n",
    "HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
    "MODEL_ID = \"microsoft/phi-2\"  # CPU-optimized model\n",
    "\n",
    "print(f\"\\nInitializing LAiSER with {MODEL_ID}...\")\n",
    "\n",
    "# Initialize LAiSER\n",
    "se = Skill_Extractor(\n",
    "    AI_MODEL_ID=MODEL_ID,\n",
    "    HF_TOKEN=HF_TOKEN,\n",
    "    use_gpu=False  # CPU mode for Windows compatibility\n",
    ")\n",
    "print(\"âœ“ LAiSER initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cc815",
   "metadata": {},
   "source": [
    "## Section 4: Primary Extraction with LAiSER\n",
    "\n",
    "### Process:\n",
    "1. Calls LAiSER's `extractor()` method with prepared DataFrame\n",
    "2. Processes in batches of 4 for stability\n",
    "3. Returns ~25 items per AFSC (300 total for 12 AFSCs)\n",
    "\n",
    "### Output Columns from LAiSER:\n",
    "- `Research ID` / `job_id`: Links to source AFSC\n",
    "- `Raw Skill`: Extracted skill phrase\n",
    "- `Correlation Coefficient`: Confidence score (0-1)\n",
    "- `Skill Tag`: ESCO taxonomy reference\n",
    "- `Description`: Original text (for evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "500a194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting skills with LAiSER...\n",
      "âœ“ Extraction complete in 0.21 seconds\n",
      "  Raw items extracted: 300\n",
      "  Items per AFSC: ~25\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting skills with LAiSER...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract skills\n",
    "extractions = se.extractor(\n",
    "    laiser_df,\n",
    "    id_column=\"job_id\",\n",
    "    text_columns=[\"description\"],\n",
    "    batch_size=4  # Conservative batch size for stability\n",
    ")\n",
    "\n",
    "# Normalize column names\n",
    "if \"Research ID\" in extractions.columns and \"job_id\" not in extractions.columns:\n",
    "    extractions = extractions.rename(columns={\"Research ID\": \"job_id\"})\n",
    "\n",
    "# Validate output\n",
    "required_cols = {\"job_id\", \"Raw Skill\", \"Correlation Coefficient\"}\n",
    "assert required_cols.issubset(extractions.columns), f\"Missing columns: {required_cols - set(extractions.columns)}\"\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"âœ“ Extraction complete in {elapsed_time:.2f} seconds\")\n",
    "print(f\"  Raw items extracted: {len(extractions)}\")\n",
    "print(f\"  Items per AFSC: ~{len(extractions) // len(laiser_df):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aca78",
   "metadata": {},
   "source": [
    "## Section 5: K/S/A Classification\n",
    "\n",
    "### Heuristic Approach:\n",
    "Since LAiSER extracts \"skills\" only, we apply keyword-based classification:\n",
    "- **Knowledge**: Contains words like \"understand\", \"theory\", \"principles\"\n",
    "- **Skill**: Contains action verbs like \"operate\", \"perform\", \"maintain\"\n",
    "- **Ability**: Contains capability words like \"lead\", \"manage\", \"coordinate\"\n",
    "\n",
    "### Reality Check:\n",
    "Most items classify as \"skills\" (90%+) because:\n",
    "- Military descriptions emphasize tasks/duties\n",
    "- ESCO taxonomy is action-oriented\n",
    "- Knowledge is often implicit in military documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f43c723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K/S/A Classification Results:\n",
      "  Skill: 265 (88.3%)\n",
      "  Ability: 33 (11.0%)\n",
      "  Knowledge: 2 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "def classify_ksa(text: str) -> str:\n",
    "    \"\"\"Classify extracted text as Knowledge, Skill, or Ability\"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    scores = {k: sum(1 for kw in kws if kw in t) for k, kws in KSA_PATTERNS.items()}\n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"skill\"\n",
    "\n",
    "# Apply classification\n",
    "extractions[\"ksa_type\"] = extractions[\"Raw Skill\"].astype(str).apply(classify_ksa)\n",
    "\n",
    "# Show distribution\n",
    "ksa_dist = extractions[\"ksa_type\"].value_counts()\n",
    "print(\"\\nK/S/A Classification Results:\")\n",
    "for ksa_type, count in ksa_dist.items():\n",
    "    print(f\"  {ksa_type.capitalize()}: {count} ({count/len(extractions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d385ed4",
   "metadata": {},
   "source": [
    "## Section 6: Evidence Snippet Generation\n",
    "\n",
    "### Purpose:\n",
    "Provides traceability by capturing text around extracted items.\n",
    "\n",
    "### Method:\n",
    "1. Takes first 3 words of extracted skill as anchor\n",
    "2. Searches for anchor in original text\n",
    "3. Captures Â±100 characters around match\n",
    "4. Creates snippet for audit/validation\n",
    "\n",
    "This lightweight approach avoids complex NLP while maintaining provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c13f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Evidence snippets generated\n",
      "  Snippets with evidence: 3/300\n"
     ]
    }
   ],
   "source": [
    "# Merge with original data for evidence extraction\n",
    "extractions = extractions.merge(\n",
    "    laiser_df[[\"job_id\", \"description\", \"afsc\", \"title\", \"category\"]],\n",
    "    on=\"job_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "def evidence_snippet(full_text: str, phrase: str, window: int = 100) -> str:\n",
    "    \"\"\"Extract text snippet around where skill was found\"\"\"\n",
    "    if not isinstance(full_text, str) or not isinstance(phrase, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Use first 3 words as anchor\n",
    "    words = phrase.lower().split()[:3]\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    \n",
    "    # Search for pattern\n",
    "    pat = r\"\\b\" + r\"\\s+\".join(map(re.escape, words))\n",
    "    m = re.search(pat, full_text.lower())\n",
    "    \n",
    "    if not m:\n",
    "        return \"\"\n",
    "    \n",
    "    # Extract window\n",
    "    start = max(0, m.start() - window)\n",
    "    end = min(len(full_text), m.end() + window)\n",
    "    return \"...\" + full_text[start:end] + \"...\"\n",
    "\n",
    "# Generate evidence snippets\n",
    "extractions[\"evidence_snippet\"] = extractions.apply(\n",
    "    lambda r: evidence_snippet(r[\"description\"], r[\"Raw Skill\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Evidence snippets generated\")\n",
    "print(f\"  Snippets with evidence: {(extractions['evidence_snippet'] != '').sum()}/{len(extractions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fe791",
   "metadata": {},
   "source": [
    "## Section 7: Quality Filters\n",
    "\n",
    "### Three-Stage Filtering:\n",
    "1. **Confidence Threshold**: Removes items below 0.55 correlation\n",
    "2. **Generic Stoplist**: Removes navy/maritime terms (irrelevant to Air Force)\n",
    "3. **Deduplication**: One instance per AFSC-skill combination\n",
    "\n",
    "### Impact:\n",
    "- Reduces from 300 raw extractions to ~106 high-quality items\n",
    "- Increases average confidence from 0.51 to 0.60\n",
    "- Removes obvious errors and redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "737581b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying quality filters...\n",
      "  After confidence filter (â‰¥0.55): 74/300\n",
      "  Removed 1 items containing 'navy operations'\n",
      "\n",
      "âœ“ Final count: 73 high-quality items\n",
      "  Average confidence: 0.601\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nApplying quality filters...\")\n",
    "\n",
    "# Stage 1: Confidence threshold\n",
    "initial_count = len(extractions)\n",
    "filtered = extractions[extractions[\"Correlation Coefficient\"] >= MIN_CONFIDENCE].copy()\n",
    "print(f\"  After confidence filter (â‰¥{MIN_CONFIDENCE}): {len(filtered)}/{initial_count}\")\n",
    "\n",
    "# Stage 2: Remove generic/irrelevant terms\n",
    "for term in GENERIC_STOPLIST:\n",
    "    before = len(filtered)\n",
    "    filtered = filtered[~filtered[\"Raw Skill\"].str.contains(term, case=False, na=False)]\n",
    "    if before > len(filtered):\n",
    "        print(f\"  Removed {before - len(filtered)} items containing '{term}'\")\n",
    "\n",
    "# Stage 3: Deduplicate per AFSC\n",
    "filtered = filtered.drop_duplicates(subset=[\"afsc\", \"Raw Skill\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ Final count: {len(filtered)} high-quality items\")\n",
    "print(f\"  Average confidence: {filtered['Correlation Coefficient'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c50d52",
   "metadata": {},
   "source": [
    "## Section 8: Quality Control Sample\n",
    "\n",
    "### Stratified Sampling:\n",
    "- Creates 30-item sample (or less if fewer items available)\n",
    "- Balanced across K/S/A types (10 each when possible)\n",
    "- Includes columns for manual review:\n",
    "  - `reviewer_label`: Corrected K/S/A classification\n",
    "  - `is_correct`: Validation flag\n",
    "  - `notes`: Reviewer comments\n",
    "\n",
    "This enables systematic validation of extraction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6849ed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ QC sample created: 18 items\n",
      "  K/S/A distribution in sample:\n",
      "    skill: 10\n",
      "    ability: 8\n"
     ]
    }
   ],
   "source": [
    "def make_qc_sample(df_in: pd.DataFrame, n: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Create stratified QC sample for manual review\"\"\"\n",
    "    if df_in.empty:\n",
    "        return df_in.assign(reviewer_label=\"\", is_correct=\"\", notes=\"\")\n",
    "    \n",
    "    # Target equal distribution across K/S/A\n",
    "    per_bucket = max(1, n // 3)\n",
    "    parts = []\n",
    "    \n",
    "    for k in [\"knowledge\", \"skill\", \"ability\"]:\n",
    "        sub = df_in[df_in[\"ksa_type\"] == k]\n",
    "        if not sub.empty:\n",
    "            sample_size = min(len(sub), per_bucket)\n",
    "            parts.append(sub.sample(sample_size, random_state=42))\n",
    "    \n",
    "    # Combine samples\n",
    "    qc = pd.concat(parts, ignore_index=True) if parts else df_in.head(min(n, len(df_in))).copy()\n",
    "    \n",
    "    # Add review columns\n",
    "    qc[\"reviewer_label\"] = \"\"\n",
    "    qc[\"is_correct\"] = \"\"\n",
    "    qc[\"notes\"] = \"\"\n",
    "    \n",
    "    # Select relevant columns for review\n",
    "    cols = [\"afsc\", \"title\", \"category\", \"ksa_type\", \"Raw Skill\", \n",
    "            \"Correlation Coefficient\", \"evidence_snippet\", \n",
    "            \"reviewer_label\", \"is_correct\", \"notes\"]\n",
    "    return qc[cols]\n",
    "\n",
    "# Generate QC sample\n",
    "qc_sample = make_qc_sample(filtered, n=30)\n",
    "print(f\"\\nâœ“ QC sample created: {len(qc_sample)} items\")\n",
    "print(f\"  K/S/A distribution in sample:\")\n",
    "for ksa, count in qc_sample[\"ksa_type\"].value_counts().items():\n",
    "    print(f\"    {ksa}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5207cb9",
   "metadata": {},
   "source": [
    "## Section 9: Summary Statistics and Exports\n",
    "\n",
    "### Statistics Captured:\n",
    "- Total extractions and unique AFSCs\n",
    "- Average confidence scores\n",
    "- K/S/A distribution\n",
    "- Per-AFSC breakdown\n",
    "\n",
    "### Graph Export Structure:\n",
    "- **Nodes**: AFSCs (12) + unique K/S/As (~47)\n",
    "- **Edges**: AFSCâ†’K/S/A relationships with confidence weights\n",
    "- Ready for Neo4j or other graph database import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde75887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Summary Statistics:\n",
      "  Total K/S/A: 73\n",
      "  Unique AFSCs: 12\n",
      "  Average confidence: 0.601\n",
      "  K/S/A distribution: {'skill': 65, 'ability': 8}\n",
      "\n",
      "ðŸ“ˆ Graph structure:\n",
      "  Nodes: 42\n",
      "  Edges: 73\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics\n",
    "stats = {\n",
    "    \"pipeline_run\": datetime.now().isoformat(),\n",
    "    \"total_extractions\": int(len(filtered)),\n",
    "    \"unique_afscs\": int(filtered[\"afsc\"].nunique()),\n",
    "    \"avg_confidence\": float(filtered[\"Correlation Coefficient\"].mean()),\n",
    "    \"min_confidence\": float(filtered[\"Correlation Coefficient\"].min()),\n",
    "    \"max_confidence\": float(filtered[\"Correlation Coefficient\"].max()),\n",
    "    \"ksa_distribution\": filtered[\"ksa_type\"].value_counts().to_dict(),\n",
    "    \"items_per_afsc\": {}\n",
    "}\n",
    "\n",
    "# Per-AFSC breakdown\n",
    "for af in filtered[\"afsc\"].unique():\n",
    "    sub = filtered[filtered[\"afsc\"] == af]\n",
    "    stats[\"items_per_afsc\"][af] = {\n",
    "        \"total\": int(len(sub)),\n",
    "        \"knowledge\": int((sub[\"ksa_type\"] == \"knowledge\").sum()),\n",
    "        \"skills\": int((sub[\"ksa_type\"] == \"skill\").sum()),\n",
    "        \"abilities\": int((sub[\"ksa_type\"] == \"ability\").sum()),\n",
    "        \"avg_confidence\": float(sub[\"Correlation Coefficient\"].mean())\n",
    "    }\n",
    "\n",
    "# Create graph export\n",
    "def graph_export(df_in: pd.DataFrame) -> dict:\n",
    "    \"\"\"Generate graph database structure\"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    \n",
    "    # AFSC nodes\n",
    "    for af in df_in[\"afsc\"].dropna().unique():\n",
    "        row = df_in[df_in[\"afsc\"] == af].iloc[0]\n",
    "        nodes.append({\n",
    "            \"id\": str(af),\n",
    "            \"type\": \"AFSC\",\n",
    "            \"properties\": {\n",
    "                \"title\": row.get(\"title\", \"\"),\n",
    "                \"category\": row.get(\"category\", \"\")\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # K/S/A nodes and edges\n",
    "    seen_node_ids = set()\n",
    "    for _, r in df_in.iterrows():\n",
    "        # Create unique node ID\n",
    "        node_id = f\"{r['ksa_type']}_{re.sub(r'[^a-z0-9]+', '_', r['Raw Skill'].lower())[:50]}\"\n",
    "        \n",
    "        # Add node if new\n",
    "        if node_id not in seen_node_ids:\n",
    "            nodes.append({\n",
    "                \"id\": node_id,\n",
    "                \"type\": r[\"ksa_type\"].upper(),\n",
    "                \"properties\": {\n",
    "                    \"text\": r[\"Raw Skill\"],\n",
    "                    \"confidence\": float(r[\"Correlation Coefficient\"])\n",
    "                }\n",
    "            })\n",
    "            seen_node_ids.add(node_id)\n",
    "        \n",
    "        # Add edge\n",
    "        edges.append({\n",
    "            \"source\": str(r[\"afsc\"]),\n",
    "            \"target\": node_id,\n",
    "            \"relationship\": f\"REQUIRES_{r['ksa_type'].upper()}\",\n",
    "            \"properties\": {\n",
    "                \"confidence\": float(r[\"Correlation Coefficient\"]),\n",
    "                \"evidence\": (r.get(\"evidence_snippet\", \"\") or \"\")[:200]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "graph_data = graph_export(filtered)\n",
    "\n",
    "print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(f\"  Total K/S/A: {stats['total_extractions']}\")\n",
    "print(f\"  Unique AFSCs: {stats['unique_afscs']}\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"  K/S/A distribution: {stats['ksa_distribution']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Graph structure:\")\n",
    "print(f\"  Nodes: {len(graph_data['nodes'])}\")\n",
    "print(f\"  Edges: {len(graph_data['edges'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b2df6",
   "metadata": {},
   "source": [
    "## Section 10: Save All Outputs\n",
    "\n",
    "### Files Generated:\n",
    "1. **ksa_extractions.csv**: Full filtered dataset with all K/S/A items\n",
    "2. **qc_sample.csv**: Stratified sample for quality control review\n",
    "3. **extraction_stats.json**: Performance metrics and statistics\n",
    "4. **graph_export.json**: Graph database import structure\n",
    "\n",
    "All files are saved to the configured output directory with timestamps for versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d238861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "âœ“ Successfully processed 12 AFSCs\n",
      "âœ“ Extracted and filtered 73 K/S/A items\n",
      "âœ“ Generated 18-item QC sample\n",
      "âœ“ Created graph with 42 nodes and 73 edges\n",
      "\n",
      "Outputs saved to: C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_output_simple\n",
      "  - ksa_extractions.csv\n",
      "  - qc_sample.csv\n",
      "  - extraction_stats.json\n",
      "  - graph_export.json\n",
      "\n",
      "ðŸŽ¯ Next steps:\n",
      "  1. Review QC sample for validation\n",
      "  2. Import graph structure to database\n",
      "  3. Analyze skill relationships across AFSCs\n",
      "  4. Consider domain-specific improvements\n"
     ]
    }
   ],
   "source": [
    "# Save all outputs\n",
    "out_main = OUTPUT_DIR / \"ksa_extractions.csv\"\n",
    "out_qc = OUTPUT_DIR / \"qc_sample.csv\"\n",
    "out_stats = OUTPUT_DIR / \"extraction_stats.json\"\n",
    "out_graph = OUTPUT_DIR / \"graph_export.json\"\n",
    "\n",
    "# Write files\n",
    "filtered.to_csv(out_main, index=False)\n",
    "qc_sample.to_csv(out_qc, index=False)\n",
    "\n",
    "with open(out_stats, \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "with open(out_graph, \"w\") as f:\n",
    "    json.dump(graph_data, f, indent=2)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ“ Successfully processed {len(laiser_df)} AFSCs\")\n",
    "print(f\"âœ“ Extracted and filtered {len(filtered)} K/S/A items\")\n",
    "print(f\"âœ“ Generated {len(qc_sample)}-item QC sample\")\n",
    "print(f\"âœ“ Created graph with {len(graph_data['nodes'])} nodes and {len(graph_data['edges'])} edges\")\n",
    "\n",
    "print(f\"\\nOutputs saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - {out_main.name}\")\n",
    "print(f\"  - {out_qc.name}\")\n",
    "print(f\"  - {out_stats.name}\")\n",
    "print(f\"  - {out_graph.name}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next steps:\")\n",
    "print(\"  1. Review QC sample for validation\")\n",
    "print(\"  2. Import graph structure to database\")\n",
    "print(\"  3. Analyze skill relationships across AFSCs\")\n",
    "print(\"  4. Consider domain-specific improvements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
