{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdd8719",
   "metadata": {},
   "source": [
    "# Documented K/S/A Enhancement Pipeline\n",
    "\n",
    "This notebook wraps the existing pipeline **as-is**. The full script is included in the next cell without any modifications.\n",
    "\n",
    "**Note:** Running the pipeline requires your local environment, data paths, and API keys (as the original script does)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9264e0",
   "metadata": {},
   "source": [
    "# LLM K/S/A Enhancement Pipeline  \n",
    "**Purpose:** Enhance LAiSER-extracted skills with knowledge requirements from AFSC documentation using LLMs (Claude / GPT-4).  \n",
    "\n",
    "**Author:** Kyle Hall  \n",
    "**Project:** AFSC to Civilian Skills Translation Capstone  \n",
    "**Last Updated:** October 2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ac452",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Dependencies\n",
    "This section pulls in all the Python tools the pipeline needs.\n",
    "\n",
    "**What it includes & why:**\n",
    "- **pandas** – reads/writes CSV files and helps reshape data.\n",
    "- **json, pathlib, os** – open/save files and handle paths in a cross-platform way.\n",
    "- **re (regex)** – finds patterns in text (e.g., “3.1 Knowledge” sections).\n",
    "- **hashlib** – makes short, stable hashes so we can cache results or build node IDs.\n",
    "- **datetime, time** – timestamps for logging and gentle pauses to avoid rate limits.\n",
    "- **typing** – clarifies function inputs/outputs (lists, dicts, etc.).\n",
    "- **getpass** – safely asks you for API keys on the command line.\n",
    "- **anthropic, openai** – talks to the LLMs (Claude / OpenAI models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f9e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from getpass import getpass\n",
    "\n",
    "# External dependencies (install with: pip install anthropic openai)\n",
    "import anthropic\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0b945",
   "metadata": {},
   "source": [
    "## Section 2: Configuration and Parameters\n",
    "This is the “settings panel” for the pipeline. You can update paths and knobs here without touching the core logic.\n",
    "\n",
    "**Key settings:**\n",
    "- **Paths** – where to read inputs and save outputs:\n",
    "  - `INPUT_FILE`: LAiSER CSV (skills/abilities already extracted).\n",
    "  - `CORPUS_FILE`: AFSC source texts (JSONL, one AFSC per line).\n",
    "  - `OUTPUT_DIR`: folder where all enhanced outputs go.\n",
    "- **Model options** – which LLM to use and how deterministic the replies should be:\n",
    "  - `ENHANCER_PROVIDER` (claude or openai), `CLAUDE_MODEL`, `OPENAI_MODEL`.\n",
    "  - `TEMPERATURE` low = more consistent answers.\n",
    "  - `SEED` (best-effort determinism for OpenAI; ignored if not supported).\n",
    "- **Quality thresholds** – help control precision vs coverage:\n",
    "  - `MIN_EXPLICIT_CONFIDENCE` – confidence for document-sourced knowledge.\n",
    "  - `MIN_INFERRED_CONFIDENCE` – lower bar for inference (used sparingly).\n",
    "  - `MAX_KNOWLEDGE_PER_AFSC` – cap to prevent any AFSC from flooding the dataset.\n",
    "- **Cache toggle** – `USE_CACHE` avoids repeat LLM calls (saves time/money).\n",
    "\n",
    "**Takeaway:** If something looks off in outputs, adjust thresholds or caps here first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - adjust these for your environment\n",
    "DATA_DIR = Path(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\")\n",
    "INPUT_FILE = DATA_DIR / \"ksa_output_simple\" / \"ksa_extractions.csv\"  # LAiSER output\n",
    "CORPUS_FILE = DATA_DIR / \"corpus_manual_dataset.jsonl\"  # AFSC documentation\n",
    "OUTPUT_DIR = DATA_DIR / \"ksa_enhanced\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model Configuration\n",
    "# These can be overridden via environment variables for flexibility\n",
    "ENHANCER_PROVIDER = os.getenv('ENHANCER_PROVIDER', 'claude')  # 'claude' or 'openai'\n",
    "CLAUDE_MODEL = os.getenv('CLAUDE_MODEL', 'claude-opus-4-1-20250805')  # Latest Claude model\n",
    "OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')  # Reliable OpenAI model\n",
    "TEMPERATURE = float(os.getenv('LLM_TEMPERATURE', '0.1'))  # Low temp for consistency\n",
    "SEED = int(os.getenv('LLM_SEED', '42'))  # For reproducibility (best-effort)\n",
    "\n",
    "# Quality Control Parameters\n",
    "\"\"\"\n",
    "These thresholds control the quality/quantity tradeoff:\n",
    "- MIN_EXPLICIT_CONFIDENCE: Higher = fewer but better explicit knowledge items\n",
    "- MIN_INFERRED_CONFIDENCE: Lower since inference is less certain\n",
    "- MAX_KNOWLEDGE_PER_AFSC: Prevents any single AFSC from dominating\n",
    "\"\"\"\n",
    "USE_CACHE = True  # Cache LLM responses to save API costs\n",
    "MIN_INFERRED_CONFIDENCE = 0.45  # Lower bar for inferred knowledge\n",
    "MIN_EXPLICIT_CONFIDENCE = 0.82  # High bar for document-extracted knowledge\n",
    "MAX_KNOWLEDGE_PER_AFSC = 15  # Cap to prevent flooding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49fe5bc",
   "metadata": {},
   "source": [
    "## Section 3: Provenance Enhancer Class\n",
    "This class is the “brains” that talks to the LLMs and keeps receipts (provenance).\n",
    "\n",
    "**What it does:**\n",
    "1. **Initialize LLM clients**  \n",
    "   Securely reads your API keys with `getpass` and sets up Anthropics/OpenAI clients based on `ENHANCER_PROVIDER`.\n",
    "\n",
    "2. **Cache results**  \n",
    "   Builds a small on-disk cache (JSON) keyed by AFSC + content hash so the same prompt/text doesn’t trigger another paid request.\n",
    "\n",
    "3. **Extract explicit knowledge from documents**  \n",
    "   - Uses regex to find likely “Knowledge” sections (e.g., “3.1 Knowledge …”).\n",
    "   - Sends only that snippet to the LLM with a very strict prompt:\n",
    "     - Output must be **short, noun-phrase knowledge items**.\n",
    "     - No bullets, numbers, quotes, or meta text.\n",
    "     - If nothing explicit is present, the model must return `NONE`.\n",
    "   - For each item, it also finds a small **evidence snippet** from the original text.\n",
    "   - Applies **filters** (dedupe, caps, confidence) before returning.\n",
    "\n",
    "4. **Infer knowledge from skills (only when sparse)**  \n",
    "   - If a given AFSC didn’t yield enough explicit knowledge, it looks at the **top skills** and asks the LLM: “What theory/knowledge is necessarily required to do these skills?”\n",
    "   - Labels these as **`skill_inferred`** and assigns a **lower confidence**.\n",
    "\n",
    "5. **Utility helpers**\n",
    "   - `_find_evidence` tries to pull the shortest sentence with 2+ key terms from the source text (so reviewers can verify the claim quickly).\n",
    "   - `_identify_section` figures out what heading we’re under (useful for provenance tags).\n",
    "\n",
    "**Why it matters:**  \n",
    "Everything added by the LLM is traceable (source text or skill context), repeatable (cache + deterministic settings), and controlled (filters + caps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6030c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProvenanceEnhancer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the enhancer with selected LLM provider.\n",
    "        Prompts for API keys securely (never stored in code).\n",
    "        Sets up cache for cost-efficient operation.\n",
    "        \"\"\"\n",
    "        print(\"API Authentication Required\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        self.provider = ENHANCER_PROVIDER\n",
    "        self.claude = None\n",
    "        self.openai = None\n",
    "        \n",
    "        # Initialize only the selected provider to minimize API setup\n",
    "        if self.provider == 'claude':\n",
    "            claude_key = getpass(\"Enter your Claude API key: \")\n",
    "            self.claude = anthropic.Anthropic(api_key=claude_key)\n",
    "            print(f\"✓ Claude ({CLAUDE_MODEL}) initialized\\n\")\n",
    "        elif self.provider == 'openai':\n",
    "            openai_key = getpass(\"Enter your OpenAI API key: \")\n",
    "            self.openai = OpenAI(api_key=openai_key)\n",
    "            print(f\"✓ OpenAI ({OPENAI_MODEL}) initialized\\n\")\n",
    "        else:\n",
    "            # Option to use both for comparison\n",
    "            claude_key = getpass(\"Enter your Claude API key: \")\n",
    "            openai_key = getpass(\"Enter your OpenAI API key: \")\n",
    "            self.claude = anthropic.Anthropic(api_key=claude_key)\n",
    "            self.openai = OpenAI(api_key=openai_key)\n",
    "            print(\"✓ Both providers initialized\\n\")\n",
    "        \n",
    "        self.cache = self.load_cache()\n",
    "        \n",
    "    def load_cache(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load cached LLM responses from disk.\n",
    "        Using a stable filename (not date-based) to maximize cache reuse.\n",
    "        \"\"\"\n",
    "        cache_file = OUTPUT_DIR / \"llm_cache_v2.json\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to disk after each operation to prevent loss.\"\"\"\n",
    "        cache_file = OUTPUT_DIR / \"llm_cache_v2.json\"\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.cache, f, indent=2)\n",
    "    \n",
    "    def _make_cache_key(self, prefix: str, afsc: str, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Create deterministic cache key based on content.\n",
    "        This ensures we reuse cached results for identical inputs,\n",
    "        even across different runs or days.\n",
    "        \"\"\"\n",
    "        content_hash = hashlib.md5(content[:500].encode()).hexdigest()[:8]\n",
    "        return f\"{prefix}_{afsc}_{content_hash}\"\n",
    "    \n",
    "    def extract_knowledge_from_document(self, afsc: str, doc: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract explicit knowledge requirements from AFSC documentation.\n",
    "        \n",
    "        This method:\n",
    "        1. Searches for knowledge sections using regex patterns\n",
    "        2. Sends relevant text to LLM for structured extraction\n",
    "        3. Finds evidence snippets for each knowledge item\n",
    "        4. Applies quality filters and deduplication\n",
    "        5. Returns up to MAX_KNOWLEDGE_PER_AFSC items\n",
    "        \n",
    "        Args:\n",
    "            afsc: Air Force Specialty Code\n",
    "            doc: Document dictionary with text and metadata\n",
    "            \n",
    "        Returns:\n",
    "            List of knowledge items with full provenance\n",
    "        \"\"\"\n",
    "        text = doc.get('text', '')\n",
    "        doc_id = doc.get('doc_id', '')\n",
    "        title = doc.get('title', '')\n",
    "        \n",
    "        # Check cache first to avoid redundant API calls\n",
    "        cache_key = self._make_cache_key('doc_knowledge', afsc, text)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        knowledge_items = []\n",
    "        \n",
    "        # Regex patterns to find knowledge sections in military documents\n",
    "        # These patterns match common formats in AFOCD/AFECD documents\n",
    "        patterns = [\n",
    "            r\"3\\.1\\.?\\s*Knowledge[.\\s]+(.+?)(?=\\n3\\.\\d|\\n2\\.|$)\",  # Section 3.1 Knowledge\n",
    "            r\"Knowledge[.\\s]*(?:is mandatory of|includes?|requires?)[:\\s]+(.+?)(?=\\n\\d\\.|$)\",  # Knowledge statements\n",
    "            r\"(?:principles?|theory|concepts?)\\s+of[:\\s]+(.+?)(?=\\n|\\.{2,}|$)\"  # Theory/concepts\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "            for match in matches:\n",
    "                knowledge_text = match[:1500]  # Limit length for LLM context\n",
    "                \n",
    "                # Track location in document for provenance\n",
    "                match_start = text.find(match)\n",
    "                section = self._identify_section(text, match_start)\n",
    "                page = doc.get('page_start', 0)\n",
    "                \n",
    "                # Carefully crafted prompt to get clean, specific knowledge items\n",
    "                prompt = f\"\"\"Extract ONLY explicit knowledge requirements from the text below.\n",
    "\n",
    "Text:\n",
    "{knowledge_text}\n",
    "\n",
    "Return ONLY knowledge items as plain lines.\n",
    "\n",
    "Rules:\n",
    "- Each item must be directly stated in the text (no inference).\n",
    "- 3-6 words per line, noun phrases only (no leading verbs).\n",
    "- Each item must be a complete, specific concept.\n",
    "- No numbers, bullets, markdown, quotes, or explanations.\n",
    "- No single words or fragments (bad: \"flight\", \"aircraft operating\").\n",
    "- Prefer specific phrases (good: \"aircraft operating procedures\").\n",
    "- Use lowercase, no trailing punctuation.\n",
    "- Maximum 10 lines. No duplicates.\n",
    "- If none are explicit, return: NONE\n",
    "\n",
    "Examples (format and specificity):\n",
    "aircraft operating procedures\n",
    "air navigation principles\n",
    "aviation meteorology\n",
    "weapons system capabilities\n",
    "mission planning procedures\n",
    "\n",
    "Return ONLY the knowledge phrases, nothing else.\"\"\"\n",
    "\n",
    "                # Call appropriate LLM based on configuration\n",
    "                if self.provider == 'claude' and self.claude:\n",
    "                    response = self.claude.messages.create(\n",
    "                        model=CLAUDE_MODEL,\n",
    "                        max_tokens=300,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    content = response.content[0].text\n",
    "                elif self.openai:\n",
    "                    # Handle seed parameter carefully (not all OpenAI versions support it)\n",
    "                    kwargs = {\n",
    "                        'model': OPENAI_MODEL,\n",
    "                        'messages': [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        'temperature': TEMPERATURE,\n",
    "                        'max_tokens': 300\n",
    "                    }\n",
    "                    try:\n",
    "                        kwargs['seed'] = SEED\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    response = self.openai.chat.completions.create(**kwargs)\n",
    "                    content = response.choices[0].message.content\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Parse LLM response and build knowledge items\n",
    "                if \"NONE\" not in content.upper():\n",
    "                    for line in content.strip().split('\\n'):\n",
    "                        if not line.strip() or line.startswith('Quote:'):\n",
    "                            continue\n",
    "                        \n",
    "                        # Clean the extracted text\n",
    "                        line = line.strip('- •*').strip()\n",
    "                        \n",
    "                        # Filter by length (too short = fragment, too long = sentence)\n",
    "                        if len(line) > 5 and len(line) < 100:\n",
    "                            # Find supporting evidence in original text\n",
    "                            evidence = self._find_evidence(knowledge_text, line)\n",
    "                            \n",
    "                            # Create knowledge item with full provenance\n",
    "                            knowledge_items.append({\n",
    "                                'text': line,\n",
    "                                'type': 'knowledge',\n",
    "                                'confidence': MIN_EXPLICIT_CONFIDENCE,\n",
    "                                'source_method': 'document_explicit',\n",
    "                                'evidence_snippet': evidence,\n",
    "                                'doc_id': doc_id,\n",
    "                                'title': title,\n",
    "                                'category': doc.get('category', ''),\n",
    "                                'afsc': afsc,\n",
    "                                'section': section,\n",
    "                                'page': page if page > 0 else None\n",
    "                            })\n",
    "        \n",
    "        # Deduplication: remove very similar items\n",
    "        seen_texts = set()\n",
    "        unique_items = []\n",
    "        for item in knowledge_items:\n",
    "            normalized = ' '.join(item['text'].lower().split())\n",
    "            if normalized not in seen_texts:\n",
    "                seen_texts.add(normalized)\n",
    "                unique_items.append(item)\n",
    "        \n",
    "        # Sort by specificity (longer = more specific) and cap\n",
    "        unique_items.sort(key=lambda x: len(x['text']), reverse=True)\n",
    "        unique_items = unique_items[:MAX_KNOWLEDGE_PER_AFSC]\n",
    "        \n",
    "        # Cache results and return\n",
    "        self.cache[cache_key] = unique_items\n",
    "        self.save_cache()\n",
    "        return unique_items\n",
    "    \n",
    "    def infer_knowledge_from_skills(self, afsc: str, skills_df: pd.DataFrame, doc_meta: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Infer implicit knowledge requirements from extracted skills.\n",
    "        \n",
    "        This fallback method is used when explicit knowledge extraction\n",
    "        yields few results. It analyzes the top skills to determine\n",
    "        what theoretical knowledge would be required.\n",
    "        \n",
    "        Args:\n",
    "            afsc: Air Force Specialty Code\n",
    "            skills_df: DataFrame of skills for this AFSC\n",
    "            doc_meta: Document metadata for provenance\n",
    "            \n",
    "        Returns:\n",
    "            List of inferred knowledge items with lower confidence\n",
    "        \"\"\"\n",
    "        if skills_df.empty:\n",
    "            return []\n",
    "        \n",
    "        # Select top 5 skills by confidence for inference\n",
    "        top_skills = skills_df.nlargest(5, 'confidence') if 'confidence' in skills_df else skills_df.head(5)\n",
    "        skills_text = '\\n'.join([f\"- {row.get('text', row.get('Raw Skill', ''))}\" for _, row in top_skills.iterrows()])\n",
    "        \n",
    "        # Include evidence from source skills for traceability\n",
    "        skills_evidence = '\\n'.join([f\"- {row.get('evidence_snippet', '')}\" for _, row in top_skills.iterrows() if row.get('evidence_snippet')])\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = self._make_cache_key('inferred', afsc, skills_text)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Conservative prompt for inference\n",
    "        prompt = f\"\"\"Given these verified Air Force skills for AFSC {afsc}:\n",
    "{skills_text}\n",
    "\n",
    "Supporting evidence from documents:\n",
    "{skills_evidence if skills_evidence else 'N/A'}\n",
    "\n",
    "What knowledge is NECESSARILY required to perform these skills?\n",
    "- Return ONLY knowledge that is clearly implied by the skills\n",
    "- If knowledge cannot be reliably inferred, return \"NONE\"\n",
    "- Format: Brief theoretical concepts (3-7 words each)\n",
    "- Maximum 3 items\"\"\"\n",
    "\n",
    "        # Call LLM for inference\n",
    "        if self.provider == 'claude' and self.claude:\n",
    "            response = self.claude.messages.create(\n",
    "                model=CLAUDE_MODEL,\n",
    "                max_tokens=200,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            content = response.content[0].text\n",
    "        elif self.openai:\n",
    "            kwargs = {\n",
    "                'model': OPENAI_MODEL,\n",
    "                'messages': [{\"role\": \"user\", \"content\": prompt}],\n",
    "                'temperature': TEMPERATURE,\n",
    "                'max_tokens': 200\n",
    "            }\n",
    "            try:\n",
    "                kwargs['seed'] = SEED\n",
    "            except Exception:\n",
    "                pass\n",
    "            response = self.openai.chat.completions.create(**kwargs)\n",
    "            content = response.choices[0].message.content\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        # Parse response and create inferred items with lower confidence\n",
    "        knowledge_items = []\n",
    "        if \"NONE\" not in content.upper():\n",
    "            for line in content.strip().split('\\n'):\n",
    "                line = line.strip('- •*').strip()\n",
    "                if len(line) > 5 and len(line) < 100:\n",
    "                    source_skills = [row.get('text', row.get('Raw Skill', '')) for _, row in top_skills.iterrows()]\n",
    "                    \n",
    "                    knowledge_items.append({\n",
    "                        'text': line,\n",
    "                        'type': 'knowledge',\n",
    "                        'confidence': MIN_INFERRED_CONFIDENCE,  # Lower confidence\n",
    "                        'source_method': 'skill_inferred',\n",
    "                        'evidence_snippet': f\"Inferred from skills: {', '.join(source_skills[:2])}\",\n",
    "                        'doc_id': doc_meta.get('doc_id', ''),\n",
    "                        'title': doc_meta.get('title', ''),\n",
    "                        'category': doc_meta.get('category', ''),\n",
    "                        'afsc': afsc,\n",
    "                        'section': 'inferred',\n",
    "                        'page': None,\n",
    "                        'parent_skills': source_skills  # Track lineage\n",
    "                    })\n",
    "        \n",
    "        self.cache[cache_key] = knowledge_items\n",
    "        self.save_cache()\n",
    "        return knowledge_items\n",
    "    \n",
    "    def _find_evidence(self, text: str, item: str) -> str:\n",
    "        \"\"\"\n",
    "        Find the best evidence snippet for a knowledge item.\n",
    "        \n",
    "        Prefers complete sentences containing multiple key terms\n",
    "        from the knowledge item, selecting the shortest matching\n",
    "        sentence for clarity.\n",
    "        \n",
    "        Args:\n",
    "            text: Source text to search\n",
    "            item: Knowledge item to find evidence for\n",
    "            \n",
    "        Returns:\n",
    "            Evidence snippet (max 200 chars)\n",
    "        \"\"\"\n",
    "        # Extract meaningful key terms (skip short words)\n",
    "        key_terms = [w for w in item.lower().split() if len(w) > 3][:3]\n",
    "        \n",
    "        if not key_terms:\n",
    "            return text[:150].strip() + \"...\"\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        best_sentence = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if len(sent) < 20:  # Too short to be meaningful\n",
    "                continue\n",
    "                \n",
    "            sent_lower = sent.lower()\n",
    "            # Count matching key terms\n",
    "            matches = sum(1 for term in key_terms if term in sent_lower)\n",
    "            \n",
    "            if matches >= 2:  # Prefer sentences with multiple matches\n",
    "                # Score: more matches better, shorter better\n",
    "                score = (matches * 1000) - len(sent)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_sentence = sent\n",
    "        \n",
    "        if best_sentence:\n",
    "            return best_sentence[:200]  # Cap length but keep complete\n",
    "        \n",
    "        # Fallback: context around first key term\n",
    "        for term in key_terms:\n",
    "            if term in text.lower():\n",
    "                idx = text.lower().find(term)\n",
    "                start = max(0, idx - 50)\n",
    "                end = min(len(text), idx + len(term) + 100)\n",
    "                return \"...\" + text[start:end].strip() + \"...\"\n",
    "        \n",
    "        return text[:150].strip() + \"...\"\n",
    "    \n",
    "    def _identify_section(self, text: str, position: int) -> str:\n",
    "        \"\"\"\n",
    "        Identify the document section number at a given position.\n",
    "        Used for provenance tracking.\n",
    "        \"\"\"\n",
    "        before = text[:position]\n",
    "        section_match = re.findall(r'(\\d+\\.\\d+\\.?\\s*[A-Z][^.]+)', before[-200:])\n",
    "        if section_match:\n",
    "            return section_match[-1][:20]\n",
    "        return \"3.1 Knowledge\"  # Default for knowledge sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3f13f",
   "metadata": {},
   "source": [
    "## Section 4: Quality Control Functions\n",
    "These are small helpers that harden the output.\n",
    "\n",
    "**What they do:**\n",
    "- **`cap_k_per_afsc`** – makes sure we keep at most `MAX_KNOWLEDGE_PER_AFSC` knowledge items per AFSC (prioritizing higher confidence). This keeps the dataset balanced across AFSCs.\n",
    "\n",
    "**Why it matters:**  \n",
    "Even with good prompts, LLMs can overshoot. Caps and sanity checks keep the graph readable and prevent any one AFSC from dominating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0a8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_k_per_afsc(df: pd.DataFrame, k: int = MAX_KNOWLEDGE_PER_AFSC) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Defensive cap to ensure no AFSC exceeds the knowledge limit.\n",
    "    \n",
    "    This function is applied after all processing to guarantee\n",
    "    that no single AFSC dominates the dataset, even if inference\n",
    "    added extra items.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all K/S/A items\n",
    "        k: Maximum knowledge items per AFSC\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with knowledge items capped per AFSC\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    for afsc in df['afsc'].dropna().unique():\n",
    "        sub = df[df['afsc'] == afsc]\n",
    "        # Keep top k knowledge items by confidence\n",
    "        keep_k = sub[sub['type'] == 'knowledge'].sort_values('confidence', ascending=False).head(k)\n",
    "        # Keep all non-knowledge items\n",
    "        others = sub[sub['type'] != 'knowledge']\n",
    "        blocks.append(pd.concat([keep_k, others], ignore_index=True))\n",
    "    return pd.concat(blocks, ignore_index=True) if blocks else df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1da5d0",
   "metadata": {},
   "source": [
    "## Section 5: Main Pipeline Function\n",
    "This is the orchestration layer that calls everything in order.\n",
    "\n",
    "**Step-by-step:**\n",
    "1. **Load LAiSER extractions**  \n",
    "   Reads the CSV produced by your earlier pipeline. Renames columns to a standard format and tags all rows with `source_method = 'laiser'`.\n",
    "\n",
    "2. **Load the AFSC corpus**  \n",
    "   Opens the JSONL file of AFSC source texts (the “ground truth” documents).\n",
    "\n",
    "3. **Initialize the enhancer**  \n",
    "   Sets up LLM clients and loads the cache.\n",
    "\n",
    "4. **For each AFSC:**\n",
    "   - Keep all original LAiSER rows (skills/abilities).\n",
    "   - Try to extract **explicit knowledge** from the AFSC document.\n",
    "   - If explicit is **sparse**, infer **minimal** knowledge from the top few skills.\n",
    "   - Add gentle delays to avoid API rate limiting.\n",
    "\n",
    "5. **Combine & normalize**  \n",
    "   - Merge the original + new knowledge.\n",
    "   - Ensure required columns exist (text, type, confidence, evidence, etc.).\n",
    "   - Normalize types to `skill / knowledge / ability`.\n",
    "   - Bound/clean confidence values to `[0, 1]`.\n",
    "   - Add `review_status` (explicit/inferred → `pending`; original LAiSER → `reviewed`).\n",
    "   - Dedupe by `(afsc, type, text)` and **apply per-AFSC cap**.\n",
    "\n",
    "6. **Save outputs**  \n",
    "   - `ksa_extractions_enhanced.csv` – the main, cleaned dataset.\n",
    "   - `qc_candidates.csv` – only new knowledge items that need human review.\n",
    "   - Calls the graph exporter to produce `graph_export_enhanced.json`.\n",
    "   - Writes `enhancement_stats.json` with counts and averages.\n",
    "\n",
    "7. **QC prints**  \n",
    "   A few quick printed checks (e.g., min per AFSC) to catch structural problems early.\n",
    "\n",
    "**Outputs to expect:**\n",
    "- A well-formed CSV you can analyze, visualize, or load to a database.\n",
    "- A graph JSON ready for Neo4j or other graph tooling.\n",
    "- A small QC file you can hand to reviewers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018a3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_with_provenance():\n",
    "    \"\"\"\n",
    "    Main pipeline function that orchestrates the enhancement process.\n",
    "    \n",
    "    Pipeline steps:\n",
    "    1. Load LAiSER extractions (baseline K/S/A)\n",
    "    2. Load AFSC corpus documents\n",
    "    3. Initialize LLM enhancer\n",
    "    4. For each AFSC:\n",
    "       - Extract explicit knowledge from documents\n",
    "       - Infer implicit knowledge if needed\n",
    "    5. Combine and validate all items\n",
    "    6. Apply quality controls\n",
    "    7. Generate outputs (CSV, JSON, statistics)\n",
    "    8. Run quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROPOSAL-COMPLIANT K/S/A ENHANCEMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ========== STEP 1: Load LAiSER Baseline ==========\n",
    "    print(\"\\n1. Loading LAiSER extractions...\")\n",
    "    base_df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Standardize column names for compatibility with rest of pipeline\n",
    "    base_df = base_df.rename(columns={\n",
    "        'Raw Skill': 'text',\n",
    "        'ksa_type': 'type',\n",
    "        'Correlation Coefficient': 'confidence'\n",
    "    })\n",
    "    \n",
    "    # Track source of existing items\n",
    "    base_df['source_method'] = 'laiser'\n",
    "    \n",
    "    print(f\"   Loaded {len(base_df)} items\")\n",
    "    \n",
    "    # ========== STEP 2: Load AFSC Corpus ==========\n",
    "    print(\"\\n2. Loading source corpus...\")\n",
    "    corpus = {}\n",
    "    with open(CORPUS_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            corpus[doc['afsc']] = doc\n",
    "    print(f\"   Loaded {len(corpus)} documents\")\n",
    "    \n",
    "    # ========== STEP 3: Initialize Enhancer ==========\n",
    "    print(\"\\n3. Initializing enhancement engine...\")\n",
    "    enhancer = ProvenanceEnhancer()\n",
    "    \n",
    "    # ========== STEP 4: Process Each AFSC ==========\n",
    "    all_items = []\n",
    "    new_knowledge = []\n",
    "    \n",
    "    print(\"\\n4. Extracting knowledge with provenance...\")\n",
    "    for afsc in sorted(base_df['afsc'].unique()):\n",
    "        print(f\"\\n   Processing {afsc}...\")\n",
    "        \n",
    "        # Keep existing LAiSER items\n",
    "        afsc_base = base_df[base_df['afsc'] == afsc].copy()\n",
    "        all_items.extend(afsc_base.to_dict('records'))\n",
    "        \n",
    "        # Extract explicit knowledge from document\n",
    "        doc_knowledge = []  # Initialize to prevent UnboundLocalError\n",
    "        if afsc in corpus:\n",
    "            doc_knowledge = enhancer.extract_knowledge_from_document(afsc, corpus[afsc])\n",
    "            new_knowledge.extend(doc_knowledge)\n",
    "            if len(doc_knowledge) > 0:\n",
    "                print(f\"     Found {len(doc_knowledge)} explicit knowledge items\")\n",
    "            else:\n",
    "                print(f\"     No explicit knowledge found in document\")\n",
    "        else:\n",
    "            print(f\"     No corpus document found; skipping explicit knowledge extraction\")\n",
    "        \n",
    "        # Infer knowledge from skills if explicit extraction was insufficient\n",
    "        afsc_skills = afsc_base[afsc_base['type'] == 'skill'].copy()\n",
    "        afsc_skills['confidence'] = pd.to_numeric(afsc_skills['confidence'], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        if len(afsc_skills) > 0 and len(doc_knowledge) < 2:  # Conservative inference threshold\n",
    "            doc_meta = corpus.get(afsc, {})\n",
    "            inferred = enhancer.infer_knowledge_from_skills(afsc, afsc_skills, doc_meta)\n",
    "            new_knowledge.extend(inferred)\n",
    "            print(f\"     Inferred {len(inferred)} knowledge items from skills\")\n",
    "        \n",
    "        time.sleep(0.3)  # Rate limiting to avoid API throttling\n",
    "    \n",
    "    # ========== STEP 5: Combine and Validate ==========\n",
    "    print(f\"\\n5. Combining and validating...\")\n",
    "    enhanced_df = pd.DataFrame(all_items + new_knowledge)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    required_cols = ['text', 'type', 'confidence', 'source_method', \n",
    "                     'evidence_snippet', 'doc_id', 'title', 'category', 'afsc']\n",
    "    \n",
    "    for col in required_cols:\n",
    "        if col not in enhanced_df.columns:\n",
    "            enhanced_df[col] = ''\n",
    "    \n",
    "    # Normalize types to lowercase and ensure valid values\n",
    "    enhanced_df['type'] = enhanced_df['type'].str.lower().replace({\n",
    "        'knowledge': 'knowledge',\n",
    "        'skill': 'skill',\n",
    "        'ability': 'ability'\n",
    "    }).fillna('skill')\n",
    "    \n",
    "    # Bound confidence scores to [0, 1]\n",
    "    enhanced_df['confidence'] = pd.to_numeric(enhanced_df['confidence'], errors='coerce').clip(0, 1).fillna(0.5)\n",
    "    \n",
    "    # Fill missing values\n",
    "    enhanced_df['evidence_snippet'] = enhanced_df['evidence_snippet'].fillna('')\n",
    "    enhanced_df['source_method'] = enhanced_df['source_method'].fillna('unknown')\n",
    "    \n",
    "    # Add review status for QC workflow\n",
    "    enhanced_df['review_status'] = enhanced_df.apply(\n",
    "        lambda x: 'pending' if x['source_method'] in ['skill_inferred', 'document_explicit'] else 'reviewed',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates (considering AFSC, type, and text)\n",
    "    enhanced_df = enhanced_df.drop_duplicates(subset=['afsc', 'type', 'text']).reset_index(drop=True)\n",
    "    \n",
    "    # Apply defensive cap per AFSC\n",
    "    enhanced_df = cap_k_per_afsc(enhanced_df, MAX_KNOWLEDGE_PER_AFSC)\n",
    "    \n",
    "    # Sort for readability\n",
    "    enhanced_df = enhanced_df.sort_values(\n",
    "        ['afsc', 'type', 'confidence'],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "    \n",
    "    # ========== STEP 6: Generate Outputs ==========\n",
    "    print(f\"\\n6. Saving outputs...\")\n",
    "    \n",
    "    # Main enhanced dataset\n",
    "    output_file = OUTPUT_DIR / \"ksa_extractions_enhanced.csv\"\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # QC candidates file (new knowledge items only)\n",
    "    qc_candidates = pd.DataFrame(new_knowledge)\n",
    "    if len(qc_candidates) > 0:\n",
    "        qc_candidates['review_status'] = 'pending'\n",
    "        qc_candidates['origin'] = 'enhancement'\n",
    "    qc_file = OUTPUT_DIR / \"qc_candidates.csv\"\n",
    "    qc_candidates.to_csv(qc_file, index=False)\n",
    "    \n",
    "    # Graph export for visualization\n",
    "    graph_file = OUTPUT_DIR / \"graph_export_enhanced.json\"\n",
    "    graph_data = export_graph(enhanced_df, graph_file)\n",
    "    \n",
    "    # Generate comprehensive statistics\n",
    "    mask_explicit = (enhanced_df['source_method'] == 'document_explicit')\n",
    "    mask_inferred = (enhanced_df['source_method'] == 'skill_inferred')\n",
    "    mask_laiser = (enhanced_df['source_method'] == 'laiser')\n",
    "    \n",
    "    stats = {\n",
    "        'enhancement_date': datetime.now().isoformat(),\n",
    "        'provider': ENHANCER_PROVIDER,\n",
    "        'model': CLAUDE_MODEL if ENHANCER_PROVIDER == 'claude' else OPENAI_MODEL,\n",
    "        'original_items': len(base_df),\n",
    "        'enhanced_items': len(enhanced_df),\n",
    "        'knowledge_added': len(new_knowledge),\n",
    "        'explicit_knowledge': sum(1 for k in new_knowledge if k.get('source_method') == 'document_explicit'),\n",
    "        'inferred_knowledge': sum(1 for k in new_knowledge if k.get('source_method') == 'skill_inferred'),\n",
    "        'type_distribution': enhanced_df['type'].value_counts().to_dict(),\n",
    "        'source_distribution': enhanced_df['source_method'].value_counts().to_dict(),\n",
    "        'avg_confidence': {\n",
    "            'overall': float(enhanced_df['confidence'].mean()),\n",
    "            'explicit': float(enhanced_df.loc[mask_explicit, 'confidence'].mean()) if mask_explicit.any() else 0.0,\n",
    "            'inferred': float(enhanced_df.loc[mask_inferred, 'confidence'].mean()) if mask_inferred.any() else 0.0,\n",
    "            'laiser': float(enhanced_df.loc[mask_laiser, 'confidence'].mean()) if mask_laiser.any() else 0.0\n",
    "        },\n",
    "        'graph_stats': {\n",
    "            'nodes': len(graph_data['nodes']),\n",
    "            'edges': len(graph_data['edges'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    stats_file = OUTPUT_DIR / \"enhancement_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # ========== STEP 7: Quality Checks ==========\n",
    "    print(\"\\n7. Running quality checks...\")\n",
    "    problems = []\n",
    "    \n",
    "    # Check confidence bounds\n",
    "    bad_conf = enhanced_df[(enhanced_df['confidence'] < 0) | (enhanced_df['confidence'] > 1)]\n",
    "    if not bad_conf.empty:\n",
    "        problems.append((\"confidence_out_of_bounds\", len(bad_conf)))\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_text = enhanced_df[enhanced_df['text'].astype(str).str.strip() == \"\"]\n",
    "    if not empty_text.empty:\n",
    "        problems.append((\"empty_text\", len(empty_text)))\n",
    "    \n",
    "    # Check each AFSC has minimum coverage\n",
    "    by_afsc = enhanced_df.groupby('afsc').size()\n",
    "    low_afsc = by_afsc[by_afsc < 3]\n",
    "    if not low_afsc.empty:\n",
    "        problems.append((\"afsc_with_lt3_items\", low_afsc.to_dict()))\n",
    "    \n",
    "    if problems:\n",
    "        print(f\"   QC issues found: {problems}\")\n",
    "    else:\n",
    "        print(f\"   ✓ All quality checks passed\")\n",
    "    \n",
    "    # ========== STEP 8: Print Summary ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ENHANCEMENT COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original items: {stats['original_items']}\")\n",
    "    print(f\"Enhanced items: {stats['enhanced_items']}\")\n",
    "    print(f\"Knowledge added: {stats['knowledge_added']}\")\n",
    "    print(f\"  - Explicit: {stats['explicit_knowledge']}\")\n",
    "    print(f\"  - Inferred: {stats['inferred_knowledge']}\")\n",
    "    print(f\"\\nType distribution:\")\n",
    "    for t, count in stats['type_distribution'].items():\n",
    "        print(f\"  {t}: {count} ({count/len(enhanced_df)*100:.1f}%)\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    print(f\"  - {output_file.name} (main dataset)\")\n",
    "    print(f\"  - {qc_file.name} (QC candidates)\")\n",
    "    print(f\"  - {graph_file.name} (graph export)\")\n",
    "    print(f\"  - {stats_file.name} (statistics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e2daf",
   "metadata": {},
   "source": [
    "## Section 6: Graph Export Function\n",
    "Builds a Neo4j-friendly JSON graph from the enhanced dataframe.\n",
    "\n",
    "**How it works:**\n",
    "- **Nodes**\n",
    "  - One node per AFSC: `{\"id\": \"21A\", \"type\": \"AFSC\", properties: {title, category}}`\n",
    "  - One node per K/S/A item, with a **stable hash ID** so the same text + AFSC doesn’t duplicate on reload.\n",
    "  - Each K/S/A node stores: `text`, `confidence`, `source_method`, and any ESCO tag found.\n",
    "\n",
    "- **Edges**\n",
    "  - For every `(AFSC, KSA)` row, creates a `REQUIRES_{TYPE}` edge from the AFSC node to that KSA node.\n",
    "  - Edge properties include `confidence`, `source_method`, and a **truncated evidence snippet**.\n",
    "\n",
    "- **Metadata**\n",
    "  - Sums of nodes/edges and counts by type (handy for quick health checks).\n",
    "\n",
    "**Why it matters:**  \n",
    "This format is easy to import into Neo4j (via APOC or script) and lets you run queries like “show all knowledge required by AFSC 1N4”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be950bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_graph(df: pd.DataFrame, output_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Export enhanced data as graph structure for visualization.\n",
    "    \n",
    "    Creates a graph with:\n",
    "    - AFSC nodes (job codes)\n",
    "    - K/S/A nodes (requirements)\n",
    "    - REQUIRES edges (relationships)\n",
    "    \n",
    "    Each node and edge includes properties for filtering and analysis.\n",
    "    Node IDs are deterministic (hash-based) for consistency across runs.\n",
    "    \n",
    "    Args:\n",
    "        df: Enhanced dataframe with all K/S/A items\n",
    "        output_path: Path to save graph JSON\n",
    "        \n",
    "    Returns:\n",
    "        Graph data dictionary with nodes, edges, and metadata\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    seen_nodes = set()\n",
    "    \n",
    "    # Create AFSC nodes (one per unique AFSC)\n",
    "    for afsc in df['afsc'].unique():\n",
    "        if pd.notna(afsc):\n",
    "            afsc_data = df[df['afsc'] == afsc].iloc[0]\n",
    "            nodes.append({\n",
    "                'id': str(afsc),\n",
    "                'type': 'AFSC',\n",
    "                'properties': {\n",
    "                    'title': afsc_data.get('title', ''),\n",
    "                    'category': afsc_data.get('category', '')\n",
    "                }\n",
    "            })\n",
    "            seen_nodes.add(str(afsc))\n",
    "    \n",
    "    # Create K/S/A nodes and edges\n",
    "    for _, row in df.iterrows():\n",
    "        # Create stable node ID using hash (ensures consistency across runs)\n",
    "        text_for_hash = f\"{row['afsc']}_{row['text']}\"\n",
    "        node_id = f\"{row['type']}_{hashlib.md5(text_for_hash.encode()).hexdigest()[:12]}\"\n",
    "        \n",
    "        # Add node if not already created\n",
    "        if node_id not in seen_nodes:\n",
    "            # Flexible ESCO column handling (different formats possible)\n",
    "            esco_tag = (row.get('esco_tag') or row.get('Skill Tag') or \n",
    "                       row.get('ESCO') or row.get('Esco Tag') or '')\n",
    "            \n",
    "            nodes.append({\n",
    "                'id': node_id,\n",
    "                'type': row['type'].upper(),\n",
    "                'properties': {\n",
    "                    'text': row['text'],\n",
    "                    'confidence': float(row['confidence']),\n",
    "                    'source_method': row.get('source_method', 'unknown'),\n",
    "                    'esco_tag': esco_tag\n",
    "                }\n",
    "            })\n",
    "            seen_nodes.add(node_id)\n",
    "        \n",
    "        # Create edge from AFSC to K/S/A\n",
    "        edges.append({\n",
    "            'source': str(row['afsc']),\n",
    "            'target': node_id,\n",
    "            'relationship': f\"REQUIRES_{row['type'].upper()}\",\n",
    "            'properties': {\n",
    "                'confidence': float(row['confidence']),\n",
    "                'evidence': (row.get('evidence_snippet', '') or '')[:200],\n",
    "                'source_method': row.get('source_method', 'unknown')\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Create graph structure with metadata\n",
    "    graph_data = {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'metadata': {\n",
    "            'created': datetime.now().isoformat(),\n",
    "            'total_nodes': len(nodes),\n",
    "            'total_edges': len(edges),\n",
    "            'afsc_count': len([n for n in nodes if n['type'] == 'AFSC']),\n",
    "            'knowledge_count': len([n for n in nodes if n['type'] == 'KNOWLEDGE']),\n",
    "            'skill_count': len([n for n in nodes if n['type'] == 'SKILL']),\n",
    "            'ability_count': len([n for n in nodes if n['type'] == 'ABILITY'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(graph_data, f, indent=2)\n",
    "    \n",
    "    return graph_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c77ce",
   "metadata": {},
   "source": [
    "## Section 7: Entry Point\n",
    "This is the standard Python “main” hook.  \n",
    "When you run the file directly (`python documented_ksa_pipeline.py`), it calls `enhance_with_provenance()`.\n",
    "\n",
    "**Tip (Jupyter users):**  \n",
    "In a notebook, you don’t need the entry point — just run the cell that defines `enhance_with_provenance()` and then call it in a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7165a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROPOSAL-COMPLIANT K/S/A ENHANCEMENT\n",
      "============================================================\n",
      "\n",
      "1. Loading LAiSER extractions...\n",
      "   Loaded 73 items\n",
      "\n",
      "2. Loading source corpus...\n",
      "   Loaded 12 documents\n",
      "\n",
      "3. Initializing enhancement engine...\n",
      "API Authentication Required\n",
      "----------------------------------------\n",
      "✓ Claude (claude-opus-4-1-20250805) initialized\n",
      "\n",
      "\n",
      "4. Extracting knowledge with provenance...\n",
      "\n",
      "   Processing 11F3...\n",
      "     Found 7 explicit knowledge items\n",
      "\n",
      "   Processing 12B...\n",
      "     Found 7 explicit knowledge items\n",
      "\n",
      "   Processing 14F...\n",
      "     Found 8 explicit knowledge items\n",
      "\n",
      "   Processing 14N...\n",
      "     Found 11 explicit knowledge items\n",
      "\n",
      "   Processing 1A3X1...\n",
      "     Found 15 explicit knowledge items\n",
      "\n",
      "   Processing 1C3...\n",
      "     Found 14 explicit knowledge items\n",
      "\n",
      "   Processing 1N0...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "   Processing 1N4...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "   Processing 21A...\n",
      "     Found 14 explicit knowledge items\n",
      "\n",
      "   Processing 21M...\n",
      "     No explicit knowledge found in document\n",
      "     Inferred 3 knowledge items from skills\n",
      "\n",
      "   Processing 2A3...\n",
      "     Found 12 explicit knowledge items\n",
      "\n",
      "   Processing 2A5...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "5. Combining and validating...\n",
      "\n",
      "6. Saving outputs...\n",
      "\n",
      "7. Running quality checks...\n",
      "   ✓ All quality checks passed\n",
      "\n",
      "============================================================\n",
      "ENHANCEMENT COMPLETE\n",
      "============================================================\n",
      "Original items: 73\n",
      "Enhanced items: 194\n",
      "Knowledge added: 121\n",
      "  - Explicit: 118\n",
      "  - Inferred: 3\n",
      "\n",
      "Type distribution:\n",
      "  knowledge: 121 (62.4%)\n",
      "  skill: 65 (33.5%)\n",
      "  ability: 8 (4.1%)\n",
      "\n",
      "Files saved:\n",
      "  - ksa_extractions_enhanced.csv (main dataset)\n",
      "  - qc_candidates.csv (QC candidates)\n",
      "  - graph_export_enhanced.json (graph export)\n",
      "  - enhancement_stats.json (statistics)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    enhance_with_provenance()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
