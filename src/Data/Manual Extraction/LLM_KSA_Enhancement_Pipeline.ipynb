{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04c3323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROPOSAL-COMPLIANT K/S/A ENHANCEMENT\n",
      "============================================================\n",
      "\n",
      "1. Loading LAiSER extractions...\n",
      "   Loaded 73 items\n",
      "\n",
      "2. Loading source corpus...\n",
      "   Loaded 12 documents\n",
      "\n",
      "3. Initializing enhancement engine...\n",
      "API Authentication Required\n",
      "----------------------------------------\n",
      "✓ Claude (claude-opus-4-1-20250805) initialized\n",
      "\n",
      "\n",
      "4. Extracting knowledge with provenance...\n",
      "\n",
      "   Processing 11F3...\n",
      "     Found 7 explicit knowledge items\n",
      "\n",
      "   Processing 12B...\n",
      "     Found 7 explicit knowledge items\n",
      "\n",
      "   Processing 14F...\n",
      "     Found 8 explicit knowledge items\n",
      "\n",
      "   Processing 14N...\n",
      "     Found 11 explicit knowledge items\n",
      "\n",
      "   Processing 1A3X1...\n",
      "     Found 15 explicit knowledge items\n",
      "\n",
      "   Processing 1C3...\n",
      "     Found 14 explicit knowledge items\n",
      "\n",
      "   Processing 1N0...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "   Processing 1N4...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "   Processing 21A...\n",
      "     Found 14 explicit knowledge items\n",
      "\n",
      "   Processing 21M...\n",
      "     No explicit knowledge found in document\n",
      "     Inferred 3 knowledge items from skills\n",
      "\n",
      "   Processing 2A3...\n",
      "     Found 12 explicit knowledge items\n",
      "\n",
      "   Processing 2A5...\n",
      "     Found 10 explicit knowledge items\n",
      "\n",
      "5. Combining and validating...\n",
      "\n",
      "6. Saving outputs...\n",
      "\n",
      "7. Running quality checks...\n",
      "   ✓ All quality checks passed\n",
      "\n",
      "============================================================\n",
      "ENHANCEMENT COMPLETE\n",
      "============================================================\n",
      "Original items: 73\n",
      "Enhanced items: 194\n",
      "Knowledge added: 121\n",
      "  - Explicit: 118\n",
      "  - Inferred: 3\n",
      "\n",
      "Type distribution:\n",
      "  knowledge: 121 (62.4%)\n",
      "  skill: 65 (33.5%)\n",
      "  ability: 8 (4.1%)\n",
      "\n",
      "Files saved:\n",
      "  - ksa_extractions_enhanced.csv (main dataset)\n",
      "  - qc_candidates.csv (QC candidates)\n",
      "  - graph_export_enhanced.json (graph export)\n",
      "  - enhancement_stats.json (statistics)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM K/S/A Enhancement Pipeline - Stable Working Version\n",
    "This version successfully produced 254 enhanced items with full provenance\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from getpass import getpass\n",
    "\n",
    "# pip install anthropic openai\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\")\n",
    "INPUT_FILE = DATA_DIR / \"ksa_output_simple\" / \"ksa_extractions.csv\"\n",
    "CORPUS_FILE = DATA_DIR / \"corpus_manual_dataset.jsonl\"\n",
    "OUTPUT_DIR = DATA_DIR / \"ksa_enhanced\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model Configuration (tool-agnostic)\n",
    "ENHANCER_PROVIDER = os.getenv('ENHANCER_PROVIDER', 'claude')  # claude or openai\n",
    "CLAUDE_MODEL = os.getenv('CLAUDE_MODEL', 'claude-opus-4-1-20250805')  # Claude Opus 4.1\n",
    "OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')  # Safe default for OpenAI\n",
    "TEMPERATURE = float(os.getenv('LLM_TEMPERATURE', '0.1'))  # Deterministic\n",
    "SEED = int(os.getenv('LLM_SEED', '42'))  # For reproducibility (best-effort)\n",
    "\n",
    "# Processing options\n",
    "USE_CACHE = True\n",
    "MIN_INFERRED_CONFIDENCE = 0.45  # Lower confidence for inferred items\n",
    "MIN_EXPLICIT_CONFIDENCE = 0.82  # Raised from 0.75 for better precision\n",
    "MAX_KNOWLEDGE_PER_AFSC = 15  # Reduced from 20 for tighter control\n",
    "\n",
    "# ============================================================================\n",
    "# EVIDENCE-PRESERVING ENHANCER\n",
    "# ============================================================================\n",
    "\n",
    "class ProvenanceEnhancer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with selected provider only\"\"\"\n",
    "        print(\"API Authentication Required\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        self.provider = ENHANCER_PROVIDER\n",
    "        self.claude = None\n",
    "        self.openai = None\n",
    "        \n",
    "        if self.provider == 'claude':\n",
    "            claude_key = getpass(\"Enter your Claude API key: \")\n",
    "            self.claude = anthropic.Anthropic(api_key=claude_key)\n",
    "            print(f\"✓ Claude ({CLAUDE_MODEL}) initialized\\n\")\n",
    "        elif self.provider == 'openai':\n",
    "            openai_key = getpass(\"Enter your OpenAI API key: \")\n",
    "            self.openai = OpenAI(api_key=openai_key)\n",
    "            print(f\"✓ OpenAI ({OPENAI_MODEL}) initialized\\n\")\n",
    "        else:\n",
    "            # Use both for comparison/validation\n",
    "            claude_key = getpass(\"Enter your Claude API key: \")\n",
    "            openai_key = getpass(\"Enter your OpenAI API key: \")\n",
    "            self.claude = anthropic.Anthropic(api_key=claude_key)\n",
    "            self.openai = OpenAI(api_key=openai_key)\n",
    "            print(\"✓ Both providers initialized\\n\")\n",
    "        \n",
    "        self.cache = self.load_cache()\n",
    "        \n",
    "    def load_cache(self) -> dict:\n",
    "        \"\"\"Load cached responses\"\"\"\n",
    "        cache_file = OUTPUT_DIR / \"llm_cache_v2.json\"  # Stable filename\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to disk\"\"\"\n",
    "        cache_file = OUTPUT_DIR / \"llm_cache_v2.json\"  # Stable filename\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.cache, f, indent=2)\n",
    "    \n",
    "    def _make_cache_key(self, prefix: str, afsc: str, content: str) -> str:\n",
    "        \"\"\"Create deterministic cache key including content hash\"\"\"\n",
    "        content_hash = hashlib.md5(content[:500].encode()).hexdigest()[:8]\n",
    "        return f\"{prefix}_{afsc}_{content_hash}\"\n",
    "    \n",
    "    def extract_knowledge_from_document(self, afsc: str, doc: Dict) -> List[Dict]:\n",
    "        \"\"\"Extract knowledge with full provenance from document\"\"\"\n",
    "        \n",
    "        text = doc.get('text', '')\n",
    "        doc_id = doc.get('doc_id', '')\n",
    "        title = doc.get('title', '')\n",
    "        \n",
    "        # Cache key includes doc content\n",
    "        cache_key = self._make_cache_key('doc_knowledge', afsc, text)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        knowledge_items = []\n",
    "        \n",
    "        # Pattern 1: Find explicit Knowledge section (improved regex)\n",
    "        patterns = [\n",
    "            r\"3\\.1\\.?\\s*Knowledge[.\\s]+(.+?)(?=\\n3\\.\\d|\\n2\\.|$)\",\n",
    "            r\"Knowledge[.\\s]*(?:is mandatory of|includes?|requires?)[:\\s]+(.+?)(?=\\n\\d\\.|$)\",\n",
    "            r\"(?:principles?|theory|concepts?)\\s+of[:\\s]+(.+?)(?=\\n|\\.{2,}|$)\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "            for match in matches:\n",
    "                knowledge_text = match[:1500]  # Limit length\n",
    "                \n",
    "                # Find exact location in document\n",
    "                match_start = text.find(match)\n",
    "                section = self._identify_section(text, match_start)\n",
    "                page = doc.get('page_start', 0)  # Use actual page if available\n",
    "                \n",
    "                # Use LLM to structure with evidence requirement\n",
    "                prompt = f\"\"\"Extract ONLY explicit knowledge requirements from the text below.\n",
    "\n",
    "Text:\n",
    "{knowledge_text}\n",
    "\n",
    "Return ONLY knowledge items as plain lines.\n",
    "\n",
    "Rules:\n",
    "- Each item must be directly stated in the text (no inference).\n",
    "- 3-6 words per line, noun phrases only (no leading verbs).\n",
    "- Each item must be a complete, specific concept.\n",
    "- No numbers, bullets, markdown, quotes, or explanations.\n",
    "- No single words or fragments (bad: \"flight\", \"aircraft operating\").\n",
    "- Prefer specific phrases (good: \"aircraft operating procedures\").\n",
    "- Use lowercase, no trailing punctuation.\n",
    "- Maximum 10 lines. No duplicates.\n",
    "- If none are explicit, return: NONE\n",
    "\n",
    "Examples (format and specificity):\n",
    "aircraft operating procedures\n",
    "air navigation principles\n",
    "aviation meteorology\n",
    "weapons system capabilities\n",
    "mission planning procedures\n",
    "\n",
    "Return ONLY the knowledge phrases, nothing else.\"\"\"\n",
    "\n",
    "                if self.provider == 'claude' and self.claude:\n",
    "                    response = self.claude.messages.create(\n",
    "                        model=CLAUDE_MODEL,\n",
    "                        max_tokens=300,\n",
    "                        temperature=TEMPERATURE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    content = response.content[0].text\n",
    "                elif self.openai:\n",
    "                    # Fix 4: Handle seed parameter carefully\n",
    "                    kwargs = {\n",
    "                        'model': OPENAI_MODEL,\n",
    "                        'messages': [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        'temperature': TEMPERATURE,\n",
    "                        'max_tokens': 300\n",
    "                    }\n",
    "                    try:\n",
    "                        kwargs['seed'] = SEED\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    response = self.openai.chat.completions.create(**kwargs)\n",
    "                    content = response.choices[0].message.content\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Parse response with evidence\n",
    "                if \"NONE\" not in content.upper():\n",
    "                    for line in content.strip().split('\\n'):\n",
    "                        if not line.strip() or line.startswith('Quote:'):\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract the knowledge item\n",
    "                        line = line.strip('- •*').strip()\n",
    "                        if len(line) > 5 and len(line) < 100:\n",
    "                            # Find supporting evidence in original text\n",
    "                            evidence = self._find_evidence(knowledge_text, line)\n",
    "                            \n",
    "                            knowledge_items.append({\n",
    "                                'text': line,\n",
    "                                'type': 'knowledge',\n",
    "                                'confidence': MIN_EXPLICIT_CONFIDENCE,\n",
    "                                'source_method': 'document_explicit',\n",
    "                                'evidence_snippet': evidence,\n",
    "                                'doc_id': doc_id,\n",
    "                                'title': title,\n",
    "                                'category': doc.get('category', ''),\n",
    "                                'afsc': afsc,\n",
    "                                'section': section,\n",
    "                                'page': page if page > 0 else None\n",
    "                            })\n",
    "        \n",
    "        # Simple deduplication: remove items with very similar text\n",
    "        seen_texts = set()\n",
    "        unique_items = []\n",
    "        for item in knowledge_items:\n",
    "            # Normalize for comparison (lowercase, strip extra spaces)\n",
    "            normalized = ' '.join(item['text'].lower().split())\n",
    "            if normalized not in seen_texts:\n",
    "                seen_texts.add(normalized)\n",
    "                unique_items.append(item)\n",
    "        \n",
    "        # Sort by text length (prefer specific over generic) and take top N\n",
    "        unique_items.sort(key=lambda x: len(x['text']), reverse=True)\n",
    "        unique_items = unique_items[:MAX_KNOWLEDGE_PER_AFSC]\n",
    "        \n",
    "        # Cache and return\n",
    "        self.cache[cache_key] = unique_items\n",
    "        self.save_cache()\n",
    "        return unique_items\n",
    "    \n",
    "    def infer_knowledge_from_skills(self, afsc: str, skills_df: pd.DataFrame, doc_meta: Dict) -> List[Dict]:\n",
    "        \"\"\"Infer knowledge with traceable provenance to source skills\n",
    "        \n",
    "        Args:\n",
    "            afsc: AFSC code\n",
    "            skills_df: DataFrame of skills for this AFSC\n",
    "            doc_meta: Document metadata from corpus for proper linkage\n",
    "        \"\"\"\n",
    "        \n",
    "        if skills_df.empty:\n",
    "            return []\n",
    "        \n",
    "        # Take top 5 skills by confidence\n",
    "        top_skills = skills_df.nlargest(5, 'confidence') if 'confidence' in skills_df else skills_df.head(5)\n",
    "        skills_text = '\\n'.join([f\"- {row.get('text', row.get('Raw Skill', ''))}\" for _, row in top_skills.iterrows()])\n",
    "        \n",
    "        # Include evidence from source skills\n",
    "        skills_evidence = '\\n'.join([f\"- {row.get('evidence_snippet', '')}\" for _, row in top_skills.iterrows() if row.get('evidence_snippet')])\n",
    "        \n",
    "        cache_key = self._make_cache_key('inferred', afsc, skills_text)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        prompt = f\"\"\"Given these verified Air Force skills for AFSC {afsc}:\n",
    "{skills_text}\n",
    "\n",
    "Supporting evidence from documents:\n",
    "{skills_evidence if skills_evidence else 'N/A'}\n",
    "\n",
    "What knowledge is NECESSARILY required to perform these skills?\n",
    "- Return ONLY knowledge that is clearly implied by the skills\n",
    "- If knowledge cannot be reliably inferred, return \"NONE\"\n",
    "- Format: Brief theoretical concepts (3-7 words each)\n",
    "- Maximum 3 items\"\"\"\n",
    "\n",
    "        if self.provider == 'claude' and self.claude:\n",
    "            response = self.claude.messages.create(\n",
    "                model=CLAUDE_MODEL,\n",
    "                max_tokens=200,\n",
    "                temperature=TEMPERATURE,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            content = response.content[0].text\n",
    "        elif self.openai:\n",
    "            # Fix 4: Handle seed parameter carefully\n",
    "            kwargs = {\n",
    "                'model': OPENAI_MODEL,\n",
    "                'messages': [{\"role\": \"user\", \"content\": prompt}],\n",
    "                'temperature': TEMPERATURE,\n",
    "                'max_tokens': 200\n",
    "            }\n",
    "            try:\n",
    "                kwargs['seed'] = SEED\n",
    "            except Exception:\n",
    "                pass\n",
    "            response = self.openai.chat.completions.create(**kwargs)\n",
    "            content = response.choices[0].message.content\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        knowledge_items = []\n",
    "        if \"NONE\" not in content.upper():\n",
    "            for line in content.strip().split('\\n'):\n",
    "                line = line.strip('- •*').strip()\n",
    "                if len(line) > 5 and len(line) < 100:\n",
    "                    # Link to source skills\n",
    "                    source_skills = [row.get('text', row.get('Raw Skill', '')) for _, row in top_skills.iterrows()]\n",
    "                    \n",
    "                    knowledge_items.append({\n",
    "                        'text': line,\n",
    "                        'type': 'knowledge',\n",
    "                        'confidence': MIN_INFERRED_CONFIDENCE,\n",
    "                        'source_method': 'skill_inferred',\n",
    "                        'evidence_snippet': f\"Inferred from skills: {', '.join(source_skills[:2])}\",\n",
    "                        'doc_id': doc_meta.get('doc_id', ''),  # Use corpus doc metadata\n",
    "                        'title': doc_meta.get('title', ''),\n",
    "                        'category': doc_meta.get('category', ''),\n",
    "                        'afsc': afsc,\n",
    "                        'section': 'inferred',\n",
    "                        'page': None,  # Use None instead of 0\n",
    "                        'parent_skills': source_skills\n",
    "                    })\n",
    "        \n",
    "        self.cache[cache_key] = knowledge_items\n",
    "        self.save_cache()\n",
    "        return knowledge_items\n",
    "    \n",
    "    def _find_evidence(self, text: str, item: str) -> str:\n",
    "        \"\"\"Find supporting evidence - prefer shortest complete sentence with ≥2 key terms\"\"\"\n",
    "        # Extract key terms (remove common words like 'of', 'the')\n",
    "        key_terms = [w for w in item.lower().split() if len(w) > 3][:3]\n",
    "        \n",
    "        if not key_terms:\n",
    "            return text[:150].strip() + \"...\"\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        best_sentence = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if len(sent) < 20:  # Too short to be meaningful\n",
    "                continue\n",
    "                \n",
    "            sent_lower = sent.lower()\n",
    "            # Count matching key terms\n",
    "            matches = sum(1 for term in key_terms if term in sent_lower)\n",
    "            \n",
    "            if matches >= 2:  # Prefer sentences with multiple key terms\n",
    "                # Score: more matches is better, shorter is better\n",
    "                score = (matches * 1000) - len(sent)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_sentence = sent\n",
    "        \n",
    "        if best_sentence:\n",
    "            return best_sentence[:200]  # Cap length but keep complete\n",
    "        \n",
    "        # Fallback: context around first key term match\n",
    "        for term in key_terms:\n",
    "            if term in text.lower():\n",
    "                idx = text.lower().find(term)\n",
    "                start = max(0, idx - 50)\n",
    "                end = min(len(text), idx + len(term) + 100)\n",
    "                return \"...\" + text[start:end].strip() + \"...\"\n",
    "        \n",
    "        return text[:150].strip() + \"...\"\n",
    "    \n",
    "    def _identify_section(self, text: str, position: int) -> str:\n",
    "        \"\"\"Identify section number at position in text\"\"\"\n",
    "        # Look backwards for section marker\n",
    "        before = text[:position]\n",
    "        section_match = re.findall(r'(\\d+\\.\\d+\\.?\\s*[A-Z][^.]+)', before[-200:])\n",
    "        if section_match:\n",
    "            return section_match[-1][:20]\n",
    "        return \"3.1 Knowledge\"  # Better default\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE WITH PROVENANCE\n",
    "# ============================================================================\n",
    "\n",
    "def cap_k_per_afsc(df: pd.DataFrame, k: int = MAX_KNOWLEDGE_PER_AFSC) -> pd.DataFrame:\n",
    "    \"\"\"Defensive cap to ensure no AFSC exceeds knowledge limit\"\"\"\n",
    "    blocks = []\n",
    "    for afsc in df['afsc'].dropna().unique():\n",
    "        sub = df[df['afsc'] == afsc]\n",
    "        keep_k = sub[sub['type'] == 'knowledge'].sort_values('confidence', ascending=False).head(k)\n",
    "        others = sub[sub['type'] != 'knowledge']\n",
    "        blocks.append(pd.concat([keep_k, others], ignore_index=True))\n",
    "    return pd.concat(blocks, ignore_index=True) if blocks else df\n",
    "\n",
    "def enhance_with_provenance():\n",
    "    \"\"\"Enhanced pipeline maintaining full provenance\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROPOSAL-COMPLIANT K/S/A ENHANCEMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load base extractions\n",
    "    print(\"\\n1. Loading LAiSER extractions...\")\n",
    "    base_df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Standardize column names for compatibility\n",
    "    base_df = base_df.rename(columns={\n",
    "        'Raw Skill': 'text',\n",
    "        'ksa_type': 'type',\n",
    "        'Correlation Coefficient': 'confidence'\n",
    "    })\n",
    "    \n",
    "    # Add source_method for existing items\n",
    "    base_df['source_method'] = 'laiser'\n",
    "    \n",
    "    print(f\"   Loaded {len(base_df)} items\")\n",
    "    \n",
    "    # Load corpus\n",
    "    print(\"\\n2. Loading source corpus...\")\n",
    "    corpus = {}\n",
    "    with open(CORPUS_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            corpus[doc['afsc']] = doc\n",
    "    print(f\"   Loaded {len(corpus)} documents\")\n",
    "    \n",
    "    # Initialize enhancer\n",
    "    print(\"\\n3. Initializing enhancement engine...\")\n",
    "    enhancer = ProvenanceEnhancer()\n",
    "    \n",
    "    # Process each AFSC\n",
    "    all_items = []\n",
    "    new_knowledge = []\n",
    "    \n",
    "    print(\"\\n4. Extracting knowledge with provenance...\")\n",
    "    for afsc in sorted(base_df['afsc'].unique()):\n",
    "        print(f\"\\n   Processing {afsc}...\")\n",
    "        \n",
    "        # Keep existing items\n",
    "        afsc_base = base_df[base_df['afsc'] == afsc].copy()\n",
    "        all_items.extend(afsc_base.to_dict('records'))\n",
    "        \n",
    "        # Extract explicit knowledge from document\n",
    "        doc_knowledge = []  # Initialize to prevent UnboundLocalError\n",
    "        if afsc in corpus:\n",
    "            doc_knowledge = enhancer.extract_knowledge_from_document(afsc, corpus[afsc])\n",
    "            new_knowledge.extend(doc_knowledge)\n",
    "            if len(doc_knowledge) > 0:\n",
    "                print(f\"     Found {len(doc_knowledge)} explicit knowledge items\")\n",
    "            else:\n",
    "                print(f\"     No explicit knowledge found in document\")\n",
    "        else:\n",
    "            print(f\"     No corpus document found; skipping explicit knowledge extraction\")\n",
    "        \n",
    "        # Infer knowledge from skills (only if needed)\n",
    "        afsc_skills = afsc_base[afsc_base['type'] == 'skill'].copy()\n",
    "        afsc_skills['confidence'] = pd.to_numeric(afsc_skills['confidence'], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        if len(afsc_skills) > 0 and len(doc_knowledge) < 2:  # Conservative inference\n",
    "            doc_meta = corpus.get(afsc, {})  # Pass document metadata\n",
    "            inferred = enhancer.infer_knowledge_from_skills(afsc, afsc_skills, doc_meta)\n",
    "            new_knowledge.extend(inferred)\n",
    "            print(f\"     Inferred {len(inferred)} knowledge items from skills\")\n",
    "        \n",
    "        time.sleep(0.3)  # Rate limiting\n",
    "    \n",
    "    # Combine all items\n",
    "    print(f\"\\n5. Combining and validating...\")\n",
    "    enhanced_df = pd.DataFrame(all_items + new_knowledge)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    required_cols = ['text', 'type', 'confidence', 'source_method', \n",
    "                     'evidence_snippet', 'doc_id', 'title', 'category', 'afsc']\n",
    "    \n",
    "    for col in required_cols:\n",
    "        if col not in enhanced_df.columns:\n",
    "            enhanced_df[col] = ''\n",
    "    \n",
    "    # FIX 1: Normalize types and bound confidence\n",
    "    enhanced_df['type'] = enhanced_df['type'].str.lower().replace({\n",
    "        'knowledge': 'knowledge',\n",
    "        'skill': 'skill',\n",
    "        'ability': 'ability'\n",
    "    }).fillna('skill')\n",
    "    \n",
    "    enhanced_df['confidence'] = pd.to_numeric(enhanced_df['confidence'], errors='coerce').clip(0, 1).fillna(0.5)\n",
    "    \n",
    "    # Fill other missing values appropriately\n",
    "    enhanced_df['evidence_snippet'] = enhanced_df['evidence_snippet'].fillna('')\n",
    "    enhanced_df['source_method'] = enhanced_df['source_method'].fillna('unknown')\n",
    "    \n",
    "    # FIX 7: Add review status for QC workflow\n",
    "    enhanced_df['review_status'] = enhanced_df.apply(\n",
    "        lambda x: 'pending' if x['source_method'] in ['skill_inferred', 'document_explicit'] else 'reviewed',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove duplicates (Fix 2: include type in dedup)\n",
    "    enhanced_df = enhanced_df.drop_duplicates(subset=['afsc', 'type', 'text']).reset_index(drop=True)\n",
    "    \n",
    "    # Apply defensive cap per AFSC (ensures no AFSC exceeds limit)\n",
    "    enhanced_df = cap_k_per_afsc(enhanced_df, MAX_KNOWLEDGE_PER_AFSC)\n",
    "    \n",
    "    # Sort by AFSC, type, confidence\n",
    "    enhanced_df = enhanced_df.sort_values(\n",
    "        ['afsc', 'type', 'confidence'],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "    \n",
    "    # Save main output\n",
    "    print(f\"\\n6. Saving outputs...\")\n",
    "    output_file = OUTPUT_DIR / \"ksa_extractions_enhanced.csv\"\n",
    "    enhanced_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Create QC candidates file for new knowledge (with review_status and origin - Fix 7)\n",
    "    qc_candidates = pd.DataFrame(new_knowledge)\n",
    "    if len(qc_candidates) > 0:\n",
    "        qc_candidates['review_status'] = 'pending'\n",
    "        qc_candidates['origin'] = 'enhancement'\n",
    "    qc_file = OUTPUT_DIR / \"qc_candidates.csv\"\n",
    "    qc_candidates.to_csv(qc_file, index=False)\n",
    "    \n",
    "    # FIX 6: Generate graph export\n",
    "    graph_file = OUTPUT_DIR / \"graph_export_enhanced.json\"\n",
    "    graph_data = export_graph(enhanced_df, graph_file)\n",
    "    \n",
    "    # Generate statistics with improved calculation (FIX 5)\n",
    "    mask_explicit = (enhanced_df['source_method'] == 'document_explicit')\n",
    "    mask_inferred = (enhanced_df['source_method'] == 'skill_inferred')\n",
    "    mask_laiser = (enhanced_df['source_method'] == 'laiser')\n",
    "    \n",
    "    stats = {\n",
    "        'enhancement_date': datetime.now().isoformat(),\n",
    "        'provider': ENHANCER_PROVIDER,\n",
    "        'model': CLAUDE_MODEL if ENHANCER_PROVIDER == 'claude' else OPENAI_MODEL,\n",
    "        'original_items': len(base_df),\n",
    "        'enhanced_items': len(enhanced_df),\n",
    "        'knowledge_added': len(new_knowledge),\n",
    "        'explicit_knowledge': sum(1 for k in new_knowledge if k.get('source_method') == 'document_explicit'),\n",
    "        'inferred_knowledge': sum(1 for k in new_knowledge if k.get('source_method') == 'skill_inferred'),\n",
    "        'type_distribution': enhanced_df['type'].value_counts().to_dict(),\n",
    "        'source_distribution': enhanced_df['source_method'].value_counts().to_dict(),\n",
    "        'avg_confidence': {\n",
    "            'overall': float(enhanced_df['confidence'].mean()),\n",
    "            'explicit': float(enhanced_df.loc[mask_explicit, 'confidence'].mean()) if mask_explicit.any() else 0.0,\n",
    "            'inferred': float(enhanced_df.loc[mask_inferred, 'confidence'].mean()) if mask_inferred.any() else 0.0,\n",
    "            'laiser': float(enhanced_df.loc[mask_laiser, 'confidence'].mean()) if mask_laiser.any() else 0.0\n",
    "        },\n",
    "        'graph_stats': {\n",
    "            'nodes': len(graph_data['nodes']),\n",
    "            'edges': len(graph_data['edges'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    stats_file = OUTPUT_DIR / \"enhancement_stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # QC checks\n",
    "    print(\"\\n7. Running quality checks...\")\n",
    "    problems = []\n",
    "    \n",
    "    # Check confidence bounds\n",
    "    bad_conf = enhanced_df[(enhanced_df['confidence'] < 0) | (enhanced_df['confidence'] > 1)]\n",
    "    if not bad_conf.empty:\n",
    "        problems.append((\"confidence_out_of_bounds\", len(bad_conf)))\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_text = enhanced_df[enhanced_df['text'].astype(str).str.strip() == \"\"]\n",
    "    if not empty_text.empty:\n",
    "        problems.append((\"empty_text\", len(empty_text)))\n",
    "    \n",
    "    # Check each AFSC has ≥3 items\n",
    "    by_afsc = enhanced_df.groupby('afsc').size()\n",
    "    low_afsc = by_afsc[by_afsc < 3]\n",
    "    if not low_afsc.empty:\n",
    "        problems.append((\"afsc_with_lt3_items\", low_afsc.to_dict()))\n",
    "    \n",
    "    if problems:\n",
    "        print(f\"   QC issues found: {problems}\")\n",
    "    else:\n",
    "        print(f\"   ✓ All quality checks passed\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ENHANCEMENT COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original items: {stats['original_items']}\")\n",
    "    print(f\"Enhanced items: {stats['enhanced_items']}\")\n",
    "    print(f\"Knowledge added: {stats['knowledge_added']}\")\n",
    "    print(f\"  - Explicit: {stats['explicit_knowledge']}\")\n",
    "    print(f\"  - Inferred: {stats['inferred_knowledge']}\")\n",
    "    print(f\"\\nType distribution:\")\n",
    "    for t, count in stats['type_distribution'].items():\n",
    "        print(f\"  {t}: {count} ({count/len(enhanced_df)*100:.1f}%)\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    print(f\"  - {output_file.name} (main dataset)\")\n",
    "    print(f\"  - {qc_file.name} (QC candidates)\")\n",
    "    print(f\"  - {graph_file.name} (graph export)\")\n",
    "    print(f\"  - {stats_file.name} (statistics)\")\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPH EXPORT FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def export_graph(df: pd.DataFrame, output_path: Path) -> dict:\n",
    "    \"\"\"Export enhanced data as graph structure\n",
    "    \n",
    "    Args:\n",
    "        df: Enhanced dataframe with KSAs\n",
    "        output_path: Path to save graph JSON\n",
    "        \n",
    "    Returns:\n",
    "        Graph data dictionary\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    seen_nodes = set()\n",
    "    \n",
    "    # Create AFSC nodes\n",
    "    for afsc in df['afsc'].unique():\n",
    "        if pd.notna(afsc):\n",
    "            afsc_data = df[df['afsc'] == afsc].iloc[0]\n",
    "            nodes.append({\n",
    "                'id': str(afsc),\n",
    "                'type': 'AFSC',\n",
    "                'properties': {\n",
    "                    'title': afsc_data.get('title', ''),\n",
    "                    'category': afsc_data.get('category', '')\n",
    "                }\n",
    "            })\n",
    "            seen_nodes.add(str(afsc))\n",
    "    \n",
    "    # Create KSA nodes and edges\n",
    "    for _, row in df.iterrows():\n",
    "        # Create stable node ID using hash\n",
    "        text_for_hash = f\"{row['afsc']}_{row['text']}\"\n",
    "        node_id = f\"{row['type']}_{hashlib.md5(text_for_hash.encode()).hexdigest()[:12]}\"\n",
    "        \n",
    "        # Add node if not already seen\n",
    "        if node_id not in seen_nodes:\n",
    "            # Fix 3: Flexible ESCO column handling\n",
    "            esco_tag = (row.get('esco_tag') or row.get('Skill Tag') or \n",
    "                       row.get('ESCO') or row.get('Esco Tag') or '')\n",
    "            \n",
    "            nodes.append({\n",
    "                'id': node_id,\n",
    "                'type': row['type'].upper(),\n",
    "                'properties': {\n",
    "                    'text': row['text'],\n",
    "                    'confidence': float(row['confidence']),\n",
    "                    'source_method': row.get('source_method', 'unknown'),\n",
    "                    'esco_tag': esco_tag\n",
    "                }\n",
    "            })\n",
    "            seen_nodes.add(node_id)\n",
    "        \n",
    "        # Add edge from AFSC to KSA\n",
    "        edges.append({\n",
    "            'source': str(row['afsc']),\n",
    "            'target': node_id,\n",
    "            'relationship': f\"REQUIRES_{row['type'].upper()}\",\n",
    "            'properties': {\n",
    "                'confidence': float(row['confidence']),\n",
    "                'evidence': (row.get('evidence_snippet', '') or '')[:200],\n",
    "                'source_method': row.get('source_method', 'unknown')\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    graph_data = {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'metadata': {\n",
    "            'created': datetime.now().isoformat(),\n",
    "            'total_nodes': len(nodes),\n",
    "            'total_edges': len(edges),\n",
    "            'afsc_count': len([n for n in nodes if n['type'] == 'AFSC']),\n",
    "            'knowledge_count': len([n for n in nodes if n['type'] == 'KNOWLEDGE']),\n",
    "            'skill_count': len([n for n in nodes if n['type'] == 'SKILL']),\n",
    "            'ability_count': len([n for n in nodes if n['type'] == 'ABILITY'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(graph_data, f, indent=2)\n",
    "    \n",
    "    return graph_data\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    enhance_with_provenance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b41f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
