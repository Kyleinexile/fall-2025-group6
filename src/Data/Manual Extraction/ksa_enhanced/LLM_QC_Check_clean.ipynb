{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db10ebd",
   "metadata": {},
   "source": [
    "# LLM K/S/A – QC Checks (Shareable Notebook)\n",
    "\n",
    "This notebook contains quick quality-control checks for the enhanced K/S/A dataset and the exported graph.\n",
    "\n",
    "**What it does**\n",
    "- Samples knowledge items for spot-checking format/length.\n",
    "- Flags malformed/overlong items.\n",
    "- Summarizes node/edge counts from `graph_export_enhanced.json`.\n",
    "- Surfaces potential evidence issues for manual review.\n",
    "\n",
    "**How to run**\n",
    "1. Install: `pip install pandas`\n",
    "2. Update the file paths in code cells to point to your local repo (they currently reference Windows absolute paths).\n",
    "3. Run cells top-to-bottom.\n",
    "\n",
    "> Note: This copy was sanitized for sharing: outputs cleared, execution counts removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c36fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "p = r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\ksa_extractions_enhanced.csv\"\n",
    "df = pd.read_csv(p)\n",
    "\n",
    "k = df[df[\"type\"]==\"knowledge\"].copy()\n",
    "\n",
    "# 1) Fragment/too-short or too-long (after the stricter prompt these should be few)\n",
    "fragments = k[k[\"text\"].str.len().between(1, 6, inclusive=\"both\")]\n",
    "too_long = k[k[\"text\"].str.len() > 60]  # long often means \"phrase + rationale\"\n",
    "print(\"Fragments:\", len(fragments), \"Too long:\", len(too_long))\n",
    "\n",
    "# 2) Leading verbs (bad for knowledge)\n",
    "LEADING_VERBS = r'^(apply|perform|conduct|operate|manage|lead|coordinate|supervise|implement|analyze|assess|develop)\\b'\n",
    "leading_verbs = k[k[\"text\"].str.lower().str.match(LEADING_VERBS, na=False)]\n",
    "print(\"Leading-verb items:\", len(leading_verbs))\n",
    "\n",
    "# 3) Generic catch-alls (tune list if you see false positives)\n",
    "GENERIC = [\n",
    "    r'\\bpolicies and procedures\\b', r'\\bregulations\\b', r'\\bstandards\\b',\n",
    "    r'\\bsafety practices\\b', r'\\brequirements\\b', r'\\bgeneral knowledge\\b'\n",
    "]\n",
    "generic_mask = k[\"text\"].str.lower().apply(lambda s: any(re.search(g, s) for g in GENERIC))\n",
    "generics = k[generic_mask]\n",
    "print(\"Generic items:\", len(generics))\n",
    "\n",
    "# 4) Duplicates after normalization\n",
    "norm = k[\"text\"].str.lower().str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "dups = k[norm.duplicated(keep=False)].sort_values([\"afsc\",\"text\"])\n",
    "print(\"Duplicate knowledge items:\", dups.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your enhanced dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\ksa_extractions_enhanced.csv\")\n",
    "\n",
    "# Check per-AFSC knowledge distribution\n",
    "print(\"Knowledge items per AFSC:\")\n",
    "for afsc in df['afsc'].unique():\n",
    "    afsc_k = df[(df['afsc'] == afsc) & (df['type'] == 'knowledge')]\n",
    "    print(f\"\\n{afsc}: {len(afsc_k)} knowledge items\")\n",
    "    \n",
    "    # Check for generic terms\n",
    "    generic_terms = ['policies and procedures', 'applicable regulations', 'safety practices', 'standard procedures']\n",
    "    for term in generic_terms:\n",
    "        generic_items = afsc_k[afsc_k['text'].str.contains(term, case=False, na=False)]\n",
    "        if not generic_items.empty:\n",
    "            print(f\"  ⚠️ Found {len(generic_items)} items with '{term}'\")\n",
    "            for _, item in generic_items.head(2).iterrows():\n",
    "                print(f\"    - {item['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ab8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for evidence audit\n",
    "sample_afscs = {\n",
    "    'Operations': ['11F3', '12B'],      # Pick one from each\n",
    "    'Intelligence': ['14N'],\n",
    "    'Maintenance': ['21A'],\n",
    "    'Inferred': ['21M']                 # The one with inferred items\n",
    "}\n",
    "\n",
    "audit_items = []\n",
    "for category, afscs in sample_afscs.items():\n",
    "    for afsc in afscs:\n",
    "        afsc_items = df[df['afsc'] == afsc]\n",
    "        \n",
    "        # Get 2 explicit + 1 inferred (if available)\n",
    "        explicit = afsc_items[(afsc_items['type'] == 'knowledge') & \n",
    "                             (afsc_items['source_method'] == 'document_explicit')].head(2)\n",
    "        inferred = afsc_items[(afsc_items['type'] == 'knowledge') & \n",
    "                             (afsc_items['source_method'] == 'skill_inferred')].head(1)\n",
    "        \n",
    "        audit_items.append(pd.concat([explicit, inferred]))\n",
    "\n",
    "# Create audit spreadsheet\n",
    "audit_df = pd.concat(audit_items)[['afsc', 'text', 'evidence_snippet', 'confidence', 'source_method']]\n",
    "audit_df['is_theoretical'] = ''  # Add column for your review\n",
    "audit_df['evidence_matches'] = ''  # Add column for your review\n",
    "audit_df.to_csv('evidence_audit.csv', index=False)\n",
    "\n",
    "print(f\"Created evidence_audit.csv with {len(audit_df)} items to review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36291461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Correct file name\n",
    "graph_file = Path(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\graph_export_enhanced.json\")\n",
    "\n",
    "# Load graph export\n",
    "with open(graph_file, 'r', encoding='utf-8') as f:\n",
    "    graph = json.load(f)\n",
    "\n",
    "# Basic counts\n",
    "afsc_nodes = [n for n in graph['nodes'] if n['type'] == 'AFSC']\n",
    "k_nodes = [n for n in graph['nodes'] if n['type'] == 'KNOWLEDGE']\n",
    "s_nodes = [n for n in graph['nodes'] if n['type'] == 'SKILL']\n",
    "a_nodes = [n for n in graph['nodes'] if n['type'] == 'ABILITY']\n",
    "\n",
    "print(f\"AFSC nodes: {len(afsc_nodes)}\")  # Should be 12\n",
    "print(f\"Knowledge nodes: {len(k_nodes)}\")  # Should be ~161\n",
    "print(f\"Skill nodes: {len(s_nodes)}\")      # Should be ~65\n",
    "print(f\"Ability nodes: {len(a_nodes)}\")    # Should be ~8\n",
    "\n",
    "# Check edges per AFSC\n",
    "from collections import Counter\n",
    "edge_counts = Counter(e['source'] for e in graph['edges'])\n",
    "print(\"\\nEdges per AFSC:\")\n",
    "for afsc, count in edge_counts.most_common():\n",
    "    print(f\"  {afsc}: {count} edges\")\n",
    "\n",
    "# Top confidence knowledge items\n",
    "k_with_conf = [(n['properties']['text'], n['properties']['confidence']) \n",
    "               for n in k_nodes]\n",
    "k_with_conf.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 knowledge items by confidence:\")\n",
    "for text, conf in k_with_conf[:5]:\n",
    "    print(f\"  {conf:.3f}: {text[:50]}...\")\n",
    "\n",
    "# Check for any low-confidence items that slipped through\n",
    "low_conf = [n for n in k_nodes if n['properties']['confidence'] < 0.82]\n",
    "if low_conf:\n",
    "    print(f\"\\n⚠️ Found {len(low_conf)} knowledge items below 0.82 confidence\")\n",
    "else:\n",
    "    print(\"\\n✓ All knowledge items meet confidence threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\ksa_extractions_enhanced.csv\")\n",
    "\n",
    "# Identify problematic patterns\n",
    "problems = df[\n",
    "    df['text'].str.contains(r'^(Based on|Quote:|^\\d+\\.\\s*\\*\\*|Here are)', case=False, na=False) |\n",
    "    df['text'].str.len() > 100  # Knowledge items shouldn't be this long\n",
    "]\n",
    "\n",
    "print(f\"Found {len(problems)} malformed knowledge items\")\n",
    "for _, row in problems.head(10).iterrows():\n",
    "    print(f\"  {row['afsc']}: {row['text'][:60]}...\")\n",
    "\n",
    "# Clean them\n",
    "def clean_knowledge_text(text):\n",
    "    # Remove LLM meta-commentary\n",
    "    text = re.sub(r'^(Based on.*?:|Quote:\\s*|Here are.*?:|\\d+\\.\\s*)', '', text)\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)  # Remove markdown bold\n",
    "    text = re.sub(r'^\"([^\"]+)\"', r'\\1', text)  # Remove quotes\n",
    "    text = re.sub(r'\\s*-\\s*Exact quote:.*', '', text)  # Remove quote references\n",
    "    \n",
    "    # Extract just the knowledge phrase if embedded\n",
    "    if 'Knowledge is mandatory of' in text:\n",
    "        match = re.search(r'Knowledge is mandatory of[:\\s]+([^.]+)', text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df['text'] = df['text'].apply(clean_knowledge_text)\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\ksa_extractions_enhanced_cleaned.csv\", index=False)\n",
    "print(f\"Cleaned and saved {len(df)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new results\n",
    "df = pd.read_csv(r\"C:\\Users\\Kyle\\OneDrive\\Desktop\\Capstone\\fall-2025-group6\\src\\Data\\Manual Extraction\\ksa_enhanced\\ksa_extractions_enhanced.csv\")\n",
    "\n",
    "# Sample knowledge items\n",
    "knowledge = df[df['type'] == 'knowledge']\n",
    "print(\"Sample knowledge items:\")\n",
    "for _, row in knowledge.head(10).iterrows():\n",
    "    print(f\"  {row['afsc']}: {row['text']}\")\n",
    "\n",
    "# Check text length distribution\n",
    "print(f\"\\nText length stats:\")\n",
    "print(f\"  Mean: {knowledge['text'].str.len().mean():.1f} chars\")\n",
    "print(f\"  Max: {knowledge['text'].str.len().max()} chars\")\n",
    "\n",
    "# Any remaining issues?\n",
    "long_items = knowledge[knowledge['text'].str.len() > 50]\n",
    "if len(long_items) > 0:\n",
    "    print(f\"\\nItems over 50 chars: {len(long_items)}\")\n",
    "    for _, row in long_items.head(3).iterrows():\n",
    "        print(f\"  {row['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980912ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
